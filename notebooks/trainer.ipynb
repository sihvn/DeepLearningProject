{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom Densenet-121 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import custom densenet-121 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\121-layer\\\\src', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\notebooks', 'C:\\\\Python312\\\\python312.zip', 'C:\\\\Python312\\\\DLLs', 'C:\\\\Python312\\\\Lib', 'C:\\\\Python312', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv', '', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# import the py file for loading the dataset\n",
    "if \"..\\\\121-layer\\\\src\" not in sys.path:\n",
    "    sys.path.insert(0,r'..\\121-layer\\src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Size: 8484\n",
      "Validation Dataset Size: 1060\n",
      "Test Dataset Size: 1060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from custom_densenet import *\n",
    "from preprocessing import *\n",
    "from train_densenet import *\n",
    "train_dataset, val_dataset,train_loader, val_loader,test_dataset, test_loader= get_data_loaders(data_dir='../raw_data/archive/', label_file='../raw_data/archive/CXR8-selected/Data_Entry_2017_v2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dense_net(\n",
       "  (initial_setup): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (denseblock1): dense_block(\n",
       "    (dense_layer0): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer1): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer2): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer3): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer4): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer5): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans1): transition_block(\n",
       "    (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (denseblock2): dense_block(\n",
       "    (dense_layer0): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer1): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer2): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer3): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer4): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer5): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer6): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer7): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer8): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer9): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer10): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer11): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans2): transition_block(\n",
       "    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (denseblock3): dense_block(\n",
       "    (dense_layer0): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer1): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer2): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer3): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer4): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer5): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer6): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer7): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer8): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer9): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer10): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer11): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer12): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer13): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer14): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer15): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer16): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer17): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer18): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer19): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer20): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer21): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer22): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer23): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans3): transition_block(\n",
       "    (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (denseblock4): dense_block(\n",
       "    (dense_layer0): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer1): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer2): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer3): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer4): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer5): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer6): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer7): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer8): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer9): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer10): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer11): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer12): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer13): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer14): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer15): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (adaptive_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "## custom implementation\n",
    "\n",
    "model = dense_net(num_class-1, training = True) #binary so only need one output\n",
    "model.to(device)\n",
    "\n",
    "## official implementation\n",
    "# import torchvision.models.densenet\n",
    "# import torchvision\n",
    "\n",
    "# model = torchvision.models.densenet121(weights='DenseNet121_Weights.DEFAULT')\n",
    "# kernelCount = model.classifier.in_features\n",
    "# model.classifier = nn.Sequential(nn.Linear(kernelCount, num_class -1), nn.Sigmoid())\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [1/531], Loss: 0.6789, tp_sum: 4.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.6154\n",
      "Epoch [1/50], Step [21/531], Loss: 0.6233, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [1/50], Step [41/531], Loss: 0.7339, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5263\n",
      "Epoch [1/50], Step [61/531], Loss: 0.6329, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [1/50], Step [81/531], Loss: 0.7525, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [1/50], Step [101/531], Loss: 0.7139, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [1/50], Step [121/531], Loss: 0.6963, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6957\n",
      "Epoch [1/50], Step [141/531], Loss: 0.6259, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [1/50], Step [161/531], Loss: 0.7049, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [1/50], Step [181/531], Loss: 0.6216, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [1/50], Step [201/531], Loss: 0.7315, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [1/50], Step [221/531], Loss: 0.6714, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [1/50], Step [241/531], Loss: 0.7308, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [1/50], Step [261/531], Loss: 0.6358, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 0.0000, batch_f1_score: 0.5714\n",
      "Epoch [1/50], Step [281/531], Loss: 0.7525, tp_sum: 1.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.1818\n",
      "Epoch [1/50], Step [301/531], Loss: 0.6440, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [1/50], Step [321/531], Loss: 0.6793, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [1/50], Step [341/531], Loss: 0.6831, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [1/50], Step [361/531], Loss: 0.7643, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5714\n",
      "Epoch [1/50], Step [381/531], Loss: 0.6583, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [1/50], Step [401/531], Loss: 0.8049, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.4211\n",
      "Epoch [1/50], Step [421/531], Loss: 0.6458, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [1/50], Step [441/531], Loss: 0.6695, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [1/50], Step [461/531], Loss: 0.6987, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [1/50], Step [481/531], Loss: 0.7167, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n",
      "Epoch [1/50], Step [501/531], Loss: 0.6911, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [1/50], Step [521/531], Loss: 0.6275, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [2/50], Step [1/531], Loss: 0.7948, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 5.0000, batch_f1_score: 0.4211\n",
      "Epoch [2/50], Step [21/531], Loss: 0.7001, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [2/50], Step [41/531], Loss: 0.7175, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [2/50], Step [61/531], Loss: 0.6863, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [2/50], Step [81/531], Loss: 0.6904, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [2/50], Step [101/531], Loss: 0.6873, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [2/50], Step [121/531], Loss: 0.7005, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.5263\n",
      "Epoch [2/50], Step [141/531], Loss: 0.7323, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [2/50], Step [161/531], Loss: 0.7876, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.4444\n",
      "Epoch [2/50], Step [181/531], Loss: 0.6265, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [2/50], Step [201/531], Loss: 0.6108, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [2/50], Step [221/531], Loss: 0.7268, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [2/50], Step [241/531], Loss: 0.6587, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [2/50], Step [261/531], Loss: 0.6016, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [2/50], Step [281/531], Loss: 0.7500, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5714\n",
      "Epoch [2/50], Step [301/531], Loss: 0.7204, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [2/50], Step [321/531], Loss: 0.5988, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [2/50], Step [341/531], Loss: 0.6201, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8235\n",
      "Epoch [2/50], Step [361/531], Loss: 0.6995, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [2/50], Step [381/531], Loss: 0.6669, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [2/50], Step [401/531], Loss: 0.5987, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [2/50], Step [421/531], Loss: 0.7611, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.4444\n",
      "Epoch [2/50], Step [441/531], Loss: 0.6660, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [2/50], Step [461/531], Loss: 0.7369, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [2/50], Step [481/531], Loss: 0.7493, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [2/50], Step [501/531], Loss: 0.6827, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [2/50], Step [521/531], Loss: 0.6805, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.3636\n",
      "Epoch [3/50], Step [1/531], Loss: 0.6833, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [3/50], Step [21/531], Loss: 0.7071, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.4444\n",
      "Epoch [3/50], Step [41/531], Loss: 0.6359, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [3/50], Step [61/531], Loss: 0.6810, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [3/50], Step [81/531], Loss: 0.7605, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [3/50], Step [101/531], Loss: 0.6949, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [3/50], Step [121/531], Loss: 0.7793, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.2857\n",
      "Epoch [3/50], Step [141/531], Loss: 0.6443, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [3/50], Step [161/531], Loss: 0.7252, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [3/50], Step [181/531], Loss: 0.6632, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [3/50], Step [201/531], Loss: 0.7060, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [3/50], Step [221/531], Loss: 0.6357, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [3/50], Step [241/531], Loss: 0.6423, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [3/50], Step [261/531], Loss: 0.6358, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [3/50], Step [281/531], Loss: 0.6976, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [3/50], Step [301/531], Loss: 0.7373, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5714\n",
      "Epoch [3/50], Step [321/531], Loss: 0.7532, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.4286\n",
      "Epoch [3/50], Step [341/531], Loss: 0.6833, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.3077\n",
      "Epoch [3/50], Step [361/531], Loss: 0.6555, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.4286\n",
      "Epoch [3/50], Step [381/531], Loss: 0.7193, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [3/50], Step [401/531], Loss: 0.6071, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7143\n",
      "Epoch [3/50], Step [421/531], Loss: 0.6721, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [3/50], Step [441/531], Loss: 0.7338, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [3/50], Step [461/531], Loss: 0.6959, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [3/50], Step [481/531], Loss: 0.7667, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [3/50], Step [501/531], Loss: 0.6710, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [3/50], Step [521/531], Loss: 0.5839, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8421\n",
      "Epoch [4/50], Step [1/531], Loss: 0.5815, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [4/50], Step [21/531], Loss: 0.6049, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [4/50], Step [41/531], Loss: 0.7583, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.5000\n",
      "Epoch [4/50], Step [61/531], Loss: 0.6065, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [4/50], Step [81/531], Loss: 0.7296, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [4/50], Step [101/531], Loss: 0.7182, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [4/50], Step [121/531], Loss: 0.6515, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [4/50], Step [141/531], Loss: 0.6901, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [4/50], Step [161/531], Loss: 0.6754, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [4/50], Step [181/531], Loss: 0.7373, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [4/50], Step [201/531], Loss: 0.6597, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [4/50], Step [221/531], Loss: 0.6149, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [4/50], Step [241/531], Loss: 0.7213, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [4/50], Step [261/531], Loss: 0.7464, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [4/50], Step [281/531], Loss: 0.8527, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.3529\n",
      "Epoch [4/50], Step [301/531], Loss: 0.6601, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [4/50], Step [321/531], Loss: 0.6457, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [4/50], Step [341/531], Loss: 0.5918, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [4/50], Step [361/531], Loss: 0.7281, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [4/50], Step [381/531], Loss: 0.7086, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [4/50], Step [401/531], Loss: 0.6893, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [4/50], Step [421/531], Loss: 0.6860, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.4286\n",
      "Epoch [4/50], Step [441/531], Loss: 0.7225, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [4/50], Step [461/531], Loss: 0.7876, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.3750\n",
      "Epoch [4/50], Step [481/531], Loss: 0.6695, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [4/50], Step [501/531], Loss: 0.6775, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4706\n",
      "Epoch [4/50], Step [521/531], Loss: 0.6567, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.6316\n",
      "Epoch [5/50], Step [1/531], Loss: 0.7283, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 6.0000, batch_f1_score: 0.6000\n",
      "Epoch [5/50], Step [21/531], Loss: 0.6783, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7619\n",
      "Epoch [5/50], Step [41/531], Loss: 0.6617, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [5/50], Step [61/531], Loss: 0.7087, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [81/531], Loss: 0.7175, tp_sum: 8.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6957\n",
      "Epoch [5/50], Step [101/531], Loss: 0.6308, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [121/531], Loss: 0.6274, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [5/50], Step [141/531], Loss: 0.5844, tp_sum: 10.0000, fp_sum: 0.0000, fn_sum: 3.0000, batch_f1_score: 0.8696\n",
      "Epoch [5/50], Step [161/531], Loss: 0.6992, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [181/531], Loss: 0.6993, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [5/50], Step [201/531], Loss: 0.6849, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [5/50], Step [221/531], Loss: 0.8050, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 5.0000, batch_f1_score: 0.3333\n",
      "Epoch [5/50], Step [241/531], Loss: 0.6376, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.5455\n",
      "Epoch [5/50], Step [261/531], Loss: 0.7325, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.2857\n",
      "Epoch [5/50], Step [281/531], Loss: 0.6892, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [5/50], Step [301/531], Loss: 0.5719, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [5/50], Step [321/531], Loss: 0.6989, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.7000\n",
      "Epoch [5/50], Step [341/531], Loss: 0.6117, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [5/50], Step [361/531], Loss: 0.6054, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [5/50], Step [381/531], Loss: 0.6021, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [401/531], Loss: 0.7009, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [5/50], Step [421/531], Loss: 0.6835, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [5/50], Step [441/531], Loss: 0.6859, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [461/531], Loss: 0.5965, tp_sum: 9.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8182\n",
      "Epoch [5/50], Step [481/531], Loss: 0.6576, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [501/531], Loss: 0.6335, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [5/50], Step [521/531], Loss: 0.6569, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [6/50], Step [1/531], Loss: 0.6533, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [6/50], Step [21/531], Loss: 0.7406, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [6/50], Step [41/531], Loss: 0.7442, tp_sum: 8.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6957\n",
      "Epoch [6/50], Step [61/531], Loss: 0.6808, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.3750\n",
      "Epoch [6/50], Step [81/531], Loss: 0.5944, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [6/50], Step [101/531], Loss: 0.6626, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [6/50], Step [121/531], Loss: 0.7612, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.3750\n",
      "Epoch [6/50], Step [141/531], Loss: 0.5805, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [6/50], Step [161/531], Loss: 0.6646, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5333\n",
      "Epoch [6/50], Step [181/531], Loss: 0.6260, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [6/50], Step [201/531], Loss: 0.7097, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.4000\n",
      "Epoch [6/50], Step [221/531], Loss: 0.6235, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8421\n",
      "Epoch [6/50], Step [241/531], Loss: 0.5974, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [6/50], Step [261/531], Loss: 0.6798, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [6/50], Step [281/531], Loss: 0.6273, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [6/50], Step [301/531], Loss: 0.6522, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [6/50], Step [321/531], Loss: 0.7030, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [6/50], Step [341/531], Loss: 0.7228, tp_sum: 7.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [6/50], Step [361/531], Loss: 0.7187, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.3333\n",
      "Epoch [6/50], Step [381/531], Loss: 0.7118, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.4706\n",
      "Epoch [6/50], Step [401/531], Loss: 0.7393, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [6/50], Step [421/531], Loss: 0.6937, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [6/50], Step [441/531], Loss: 0.6194, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [6/50], Step [461/531], Loss: 0.5884, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [6/50], Step [481/531], Loss: 0.6987, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [6/50], Step [501/531], Loss: 0.6567, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [6/50], Step [521/531], Loss: 0.6001, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [7/50], Step [1/531], Loss: 0.7126, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.3750\n",
      "Epoch [7/50], Step [21/531], Loss: 0.6899, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [7/50], Step [41/531], Loss: 0.6771, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [7/50], Step [61/531], Loss: 0.6217, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.7000\n",
      "Epoch [7/50], Step [81/531], Loss: 0.7234, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.5882\n",
      "Epoch [7/50], Step [101/531], Loss: 0.7775, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.4211\n",
      "Epoch [7/50], Step [121/531], Loss: 0.6457, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [7/50], Step [141/531], Loss: 0.7154, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [7/50], Step [161/531], Loss: 0.7269, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [7/50], Step [181/531], Loss: 0.6863, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 7.0000, batch_f1_score: 0.5714\n",
      "Epoch [7/50], Step [201/531], Loss: 0.7381, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 7.0000, batch_f1_score: 0.4706\n",
      "Epoch [7/50], Step [221/531], Loss: 0.6248, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6667\n",
      "Epoch [7/50], Step [241/531], Loss: 0.6847, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [7/50], Step [261/531], Loss: 0.6739, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5263\n",
      "Epoch [7/50], Step [281/531], Loss: 0.8033, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 5.0000, batch_f1_score: 0.3529\n",
      "Epoch [7/50], Step [301/531], Loss: 0.5984, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.9000\n",
      "Epoch [7/50], Step [321/531], Loss: 0.7582, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [7/50], Step [341/531], Loss: 0.6681, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [7/50], Step [361/531], Loss: 0.6024, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [7/50], Step [381/531], Loss: 0.6612, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [7/50], Step [401/531], Loss: 0.6328, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [7/50], Step [421/531], Loss: 0.6005, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [7/50], Step [441/531], Loss: 0.6068, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [7/50], Step [461/531], Loss: 0.7202, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [7/50], Step [481/531], Loss: 0.7105, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [7/50], Step [501/531], Loss: 0.7237, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.6316\n",
      "Epoch [7/50], Step [521/531], Loss: 0.8107, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.3529\n",
      "Epoch [8/50], Step [1/531], Loss: 0.6194, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [8/50], Step [21/531], Loss: 0.7301, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [8/50], Step [41/531], Loss: 0.6720, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [8/50], Step [61/531], Loss: 0.6224, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5882\n",
      "Epoch [8/50], Step [81/531], Loss: 0.7039, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [8/50], Step [101/531], Loss: 0.6089, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [8/50], Step [121/531], Loss: 0.7033, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [8/50], Step [141/531], Loss: 0.6490, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [8/50], Step [161/531], Loss: 0.7640, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [8/50], Step [181/531], Loss: 0.6540, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [8/50], Step [201/531], Loss: 0.7570, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5714\n",
      "Epoch [8/50], Step [221/531], Loss: 0.6336, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8000\n",
      "Epoch [8/50], Step [241/531], Loss: 0.6649, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [8/50], Step [261/531], Loss: 0.7104, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5263\n",
      "Epoch [8/50], Step [281/531], Loss: 0.6698, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7368\n",
      "Epoch [8/50], Step [301/531], Loss: 0.6972, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6957\n",
      "Epoch [8/50], Step [321/531], Loss: 0.6944, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [8/50], Step [341/531], Loss: 0.6618, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [8/50], Step [361/531], Loss: 0.6377, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [8/50], Step [381/531], Loss: 0.6701, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [8/50], Step [401/531], Loss: 0.6976, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [8/50], Step [421/531], Loss: 0.7063, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [8/50], Step [441/531], Loss: 0.6632, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [8/50], Step [461/531], Loss: 0.5902, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7143\n",
      "Epoch [8/50], Step [481/531], Loss: 0.5964, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [8/50], Step [501/531], Loss: 0.6725, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [8/50], Step [521/531], Loss: 0.7248, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [9/50], Step [1/531], Loss: 0.6662, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [21/531], Loss: 0.6302, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [9/50], Step [41/531], Loss: 0.6701, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.4286\n",
      "Epoch [9/50], Step [61/531], Loss: 0.6933, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [9/50], Step [81/531], Loss: 0.6635, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [9/50], Step [101/531], Loss: 0.6514, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [121/531], Loss: 0.7505, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.3750\n",
      "Epoch [9/50], Step [141/531], Loss: 0.6364, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [9/50], Step [161/531], Loss: 0.5684, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [9/50], Step [181/531], Loss: 0.8024, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 7.0000, batch_f1_score: 0.4211\n",
      "Epoch [9/50], Step [201/531], Loss: 0.6179, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [9/50], Step [221/531], Loss: 0.6456, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [9/50], Step [241/531], Loss: 0.6771, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [9/50], Step [261/531], Loss: 0.6814, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [9/50], Step [281/531], Loss: 0.7130, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [9/50], Step [301/531], Loss: 0.6681, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n",
      "Epoch [9/50], Step [321/531], Loss: 0.6713, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [9/50], Step [341/531], Loss: 0.6363, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5882\n",
      "Epoch [9/50], Step [361/531], Loss: 0.7052, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [381/531], Loss: 0.6844, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [9/50], Step [401/531], Loss: 0.5986, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [421/531], Loss: 0.6500, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [9/50], Step [441/531], Loss: 0.6554, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [9/50], Step [461/531], Loss: 0.6566, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [481/531], Loss: 0.7354, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [9/50], Step [501/531], Loss: 0.6525, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [9/50], Step [521/531], Loss: 0.6163, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [10/50], Step [1/531], Loss: 0.6282, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [10/50], Step [21/531], Loss: 0.7060, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [10/50], Step [41/531], Loss: 0.6497, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [10/50], Step [61/531], Loss: 0.7004, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [10/50], Step [81/531], Loss: 0.7255, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [10/50], Step [101/531], Loss: 0.6703, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [10/50], Step [121/531], Loss: 0.6405, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.7059\n",
      "Epoch [10/50], Step [141/531], Loss: 0.7337, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [10/50], Step [161/531], Loss: 0.6101, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [10/50], Step [181/531], Loss: 0.6053, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [10/50], Step [201/531], Loss: 0.6749, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [10/50], Step [221/531], Loss: 0.6698, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [10/50], Step [241/531], Loss: 0.7136, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [10/50], Step [261/531], Loss: 0.7344, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 6.0000, batch_f1_score: 0.6364\n",
      "Epoch [10/50], Step [281/531], Loss: 0.7019, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [10/50], Step [301/531], Loss: 0.6351, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [10/50], Step [321/531], Loss: 0.6235, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [10/50], Step [341/531], Loss: 0.7048, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [10/50], Step [361/531], Loss: 0.7409, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [10/50], Step [381/531], Loss: 0.5529, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [10/50], Step [401/531], Loss: 0.6104, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [10/50], Step [421/531], Loss: 0.6990, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [10/50], Step [441/531], Loss: 0.6712, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.3636\n",
      "Epoch [10/50], Step [461/531], Loss: 0.6124, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [10/50], Step [481/531], Loss: 0.7889, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 5.0000, batch_f1_score: 0.2353\n",
      "Epoch [10/50], Step [501/531], Loss: 0.6713, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [10/50], Step [521/531], Loss: 0.5385, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [11/50], Step [1/531], Loss: 0.7429, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [11/50], Step [21/531], Loss: 0.7667, tp_sum: 1.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.1538\n",
      "Epoch [11/50], Step [41/531], Loss: 0.7262, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5714\n",
      "Epoch [11/50], Step [61/531], Loss: 0.7513, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.2857\n",
      "Epoch [11/50], Step [81/531], Loss: 0.6426, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [11/50], Step [101/531], Loss: 0.6049, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.9000\n",
      "Epoch [11/50], Step [121/531], Loss: 0.8137, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.3529\n",
      "Epoch [11/50], Step [141/531], Loss: 0.7132, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [11/50], Step [161/531], Loss: 0.6092, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8235\n",
      "Epoch [11/50], Step [181/531], Loss: 0.6377, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [11/50], Step [201/531], Loss: 0.7010, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 2.0000, batch_f1_score: 0.2857\n",
      "Epoch [11/50], Step [221/531], Loss: 0.6103, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [11/50], Step [241/531], Loss: 0.6615, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [11/50], Step [261/531], Loss: 0.6211, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [11/50], Step [281/531], Loss: 0.7903, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 5.0000, batch_f1_score: 0.2500\n",
      "Epoch [11/50], Step [301/531], Loss: 0.7827, tp_sum: 1.0000, fp_sum: 4.0000, fn_sum: 8.0000, batch_f1_score: 0.1429\n",
      "Epoch [11/50], Step [321/531], Loss: 0.6342, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [11/50], Step [341/531], Loss: 0.7014, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [11/50], Step [361/531], Loss: 0.6576, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [11/50], Step [381/531], Loss: 0.7125, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 1.0000, batch_f1_score: 0.3077\n",
      "Epoch [11/50], Step [401/531], Loss: 0.6001, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [11/50], Step [421/531], Loss: 0.6346, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8421\n",
      "Epoch [11/50], Step [441/531], Loss: 0.7318, tp_sum: 3.0000, fp_sum: 2.0000, fn_sum: 7.0000, batch_f1_score: 0.4000\n",
      "Epoch [11/50], Step [461/531], Loss: 0.6976, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [11/50], Step [481/531], Loss: 0.7095, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [11/50], Step [501/531], Loss: 0.6484, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [11/50], Step [521/531], Loss: 0.6624, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [12/50], Step [1/531], Loss: 0.6905, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [12/50], Step [21/531], Loss: 0.6813, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4706\n",
      "Epoch [12/50], Step [41/531], Loss: 0.6538, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.6667\n",
      "Epoch [12/50], Step [61/531], Loss: 0.6335, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [12/50], Step [81/531], Loss: 0.6650, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [12/50], Step [101/531], Loss: 0.6692, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [12/50], Step [121/531], Loss: 0.6473, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [12/50], Step [141/531], Loss: 0.6560, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [12/50], Step [161/531], Loss: 0.6075, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7778\n",
      "Epoch [12/50], Step [181/531], Loss: 0.5926, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [12/50], Step [201/531], Loss: 0.6183, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [12/50], Step [221/531], Loss: 0.7294, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [12/50], Step [241/531], Loss: 0.6638, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [12/50], Step [261/531], Loss: 0.6011, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [12/50], Step [281/531], Loss: 0.6384, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [12/50], Step [301/531], Loss: 0.7010, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [12/50], Step [321/531], Loss: 0.6884, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n"
     ]
    }
   ],
   "source": [
    "train(model = model,\n",
    "      train_loader = train_loader,\n",
    "      train_dataset_length = len(train_dataset),\n",
    "      val_loader = val_loader,\n",
    "      num_class = num_class,\n",
    "      device = device,\n",
    "      model_name = \"custom_dnet_binary_by_img_count_lr_1e-4_long\",\n",
    "      lr = 1e-4\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
