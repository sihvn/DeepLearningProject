{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Custom Densenet-121 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import custom densenet-121 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\121-layer\\\\src', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\notebooks', 'C:\\\\Python312\\\\python312.zip', 'C:\\\\Python312\\\\DLLs', 'C:\\\\Python312\\\\Lib', 'C:\\\\Python312', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv', '', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# import the py file for loading the dataset\n",
    "if \"..\\\\121-layer\\\\src\" not in sys.path:\n",
    "    sys.path.insert(0,r'..\\121-layer\\src')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Size: 8484\n",
      "Validation Dataset Size: 1060\n",
      "Test Dataset Size: 1060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "import random\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "from custom_densenet import *\n",
    "from preprocessing import *\n",
    "from train_densenet import *\n",
    "train_dataset, val_dataset,train_loader, val_loader,test_dataset, test_loader= get_data_loaders(data_dir='../raw_data/archive/', label_file='../raw_data/archive/CXR8-selected/Data_Entry_2017_v2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dense_net(\n",
       "  (initial_setup): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (denseblock1): dense_block(\n",
       "    (dense_layer0): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer1): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer2): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer3): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer4): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer5): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans1): transition_block(\n",
       "    (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (denseblock2): dense_block(\n",
       "    (dense_layer0): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer1): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer2): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer3): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer4): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer5): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer6): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer7): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer8): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer9): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer10): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer11): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans2): transition_block(\n",
       "    (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (denseblock3): dense_block(\n",
       "    (dense_layer0): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer1): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer2): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer3): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer4): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer5): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer6): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer7): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer8): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer9): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer10): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer11): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer12): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer13): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer14): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer15): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer16): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer17): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer18): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer19): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer20): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer21): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer22): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer23): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (trans3): transition_block(\n",
       "    (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  )\n",
       "  (denseblock4): dense_block(\n",
       "    (dense_layer0): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer1): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer2): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer3): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer4): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer5): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer6): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer7): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer8): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer9): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer10): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer11): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer12): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer13): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer14): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (dense_layer15): dense_layer(\n",
       "      (net): Sequential(\n",
       "        (0): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (4): ReLU(inplace=True)\n",
       "        (5): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (adaptive_avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = 2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "## custom implementation\n",
    "\n",
    "model = dense_net(num_class-1, training = True) #binary so only need one output\n",
    "model.to(device)\n",
    "\n",
    "## official implementation\n",
    "# import torchvision.models.densenet\n",
    "# import torchvision\n",
    "\n",
    "# model = torchvision.models.densenet121(weights='DenseNet121_Weights.DEFAULT')\n",
    "# kernelCount = model.classifier.in_features\n",
    "# model.classifier = nn.Sequential(nn.Linear(kernelCount, num_class -1), nn.Sigmoid())\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [1/531], Loss: 0.6789, tp_sum: 4.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.6154\n",
      "Epoch [1/50], Step [21/531], Loss: 0.6233, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [1/50], Step [41/531], Loss: 0.7339, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5263\n",
      "Epoch [1/50], Step [61/531], Loss: 0.6329, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [1/50], Step [81/531], Loss: 0.7525, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [1/50], Step [101/531], Loss: 0.7139, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [1/50], Step [121/531], Loss: 0.6963, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6957\n",
      "Epoch [1/50], Step [141/531], Loss: 0.6259, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [1/50], Step [161/531], Loss: 0.7049, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [1/50], Step [181/531], Loss: 0.6216, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [1/50], Step [201/531], Loss: 0.7315, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [1/50], Step [221/531], Loss: 0.6714, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [1/50], Step [241/531], Loss: 0.7308, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [1/50], Step [261/531], Loss: 0.6358, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 0.0000, batch_f1_score: 0.5714\n",
      "Epoch [1/50], Step [281/531], Loss: 0.7525, tp_sum: 1.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.1818\n",
      "Epoch [1/50], Step [301/531], Loss: 0.6440, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [1/50], Step [321/531], Loss: 0.6793, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [1/50], Step [341/531], Loss: 0.6831, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [1/50], Step [361/531], Loss: 0.7643, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5714\n",
      "Epoch [1/50], Step [381/531], Loss: 0.6583, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [1/50], Step [401/531], Loss: 0.8049, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.4211\n",
      "Epoch [1/50], Step [421/531], Loss: 0.6458, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [1/50], Step [441/531], Loss: 0.6695, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [1/50], Step [461/531], Loss: 0.6987, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [1/50], Step [481/531], Loss: 0.7167, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n",
      "Epoch [1/50], Step [501/531], Loss: 0.6911, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [1/50], Step [521/531], Loss: 0.6275, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [2/50], Step [1/531], Loss: 0.7948, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 5.0000, batch_f1_score: 0.4211\n",
      "Epoch [2/50], Step [21/531], Loss: 0.7001, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [2/50], Step [41/531], Loss: 0.7175, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [2/50], Step [61/531], Loss: 0.6863, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [2/50], Step [81/531], Loss: 0.6904, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [2/50], Step [101/531], Loss: 0.6873, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [2/50], Step [121/531], Loss: 0.7005, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.5263\n",
      "Epoch [2/50], Step [141/531], Loss: 0.7323, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [2/50], Step [161/531], Loss: 0.7876, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.4444\n",
      "Epoch [2/50], Step [181/531], Loss: 0.6265, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [2/50], Step [201/531], Loss: 0.6108, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [2/50], Step [221/531], Loss: 0.7268, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [2/50], Step [241/531], Loss: 0.6587, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [2/50], Step [261/531], Loss: 0.6016, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [2/50], Step [281/531], Loss: 0.7500, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5714\n",
      "Epoch [2/50], Step [301/531], Loss: 0.7204, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [2/50], Step [321/531], Loss: 0.5988, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [2/50], Step [341/531], Loss: 0.6201, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8235\n",
      "Epoch [2/50], Step [361/531], Loss: 0.6995, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [2/50], Step [381/531], Loss: 0.6669, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [2/50], Step [401/531], Loss: 0.5987, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [2/50], Step [421/531], Loss: 0.7611, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.4444\n",
      "Epoch [2/50], Step [441/531], Loss: 0.6660, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [2/50], Step [461/531], Loss: 0.7369, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [2/50], Step [481/531], Loss: 0.7493, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [2/50], Step [501/531], Loss: 0.6827, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [2/50], Step [521/531], Loss: 0.6805, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.3636\n",
      "Epoch [3/50], Step [1/531], Loss: 0.6833, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [3/50], Step [21/531], Loss: 0.7071, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.4444\n",
      "Epoch [3/50], Step [41/531], Loss: 0.6359, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [3/50], Step [61/531], Loss: 0.6810, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [3/50], Step [81/531], Loss: 0.7605, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [3/50], Step [101/531], Loss: 0.6949, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [3/50], Step [121/531], Loss: 0.7793, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.2857\n",
      "Epoch [3/50], Step [141/531], Loss: 0.6443, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [3/50], Step [161/531], Loss: 0.7252, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [3/50], Step [181/531], Loss: 0.6632, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [3/50], Step [201/531], Loss: 0.7060, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [3/50], Step [221/531], Loss: 0.6357, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [3/50], Step [241/531], Loss: 0.6423, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [3/50], Step [261/531], Loss: 0.6358, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [3/50], Step [281/531], Loss: 0.6976, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [3/50], Step [301/531], Loss: 0.7373, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5714\n",
      "Epoch [3/50], Step [321/531], Loss: 0.7532, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.4286\n",
      "Epoch [3/50], Step [341/531], Loss: 0.6833, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.3077\n",
      "Epoch [3/50], Step [361/531], Loss: 0.6555, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.4286\n",
      "Epoch [3/50], Step [381/531], Loss: 0.7193, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [3/50], Step [401/531], Loss: 0.6071, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7143\n",
      "Epoch [3/50], Step [421/531], Loss: 0.6721, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [3/50], Step [441/531], Loss: 0.7338, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [3/50], Step [461/531], Loss: 0.6959, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [3/50], Step [481/531], Loss: 0.7667, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [3/50], Step [501/531], Loss: 0.6710, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [3/50], Step [521/531], Loss: 0.5839, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8421\n",
      "Epoch [4/50], Step [1/531], Loss: 0.5815, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [4/50], Step [21/531], Loss: 0.6049, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [4/50], Step [41/531], Loss: 0.7583, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.5000\n",
      "Epoch [4/50], Step [61/531], Loss: 0.6065, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [4/50], Step [81/531], Loss: 0.7296, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [4/50], Step [101/531], Loss: 0.7182, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [4/50], Step [121/531], Loss: 0.6515, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [4/50], Step [141/531], Loss: 0.6901, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [4/50], Step [161/531], Loss: 0.6754, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [4/50], Step [181/531], Loss: 0.7373, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [4/50], Step [201/531], Loss: 0.6597, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [4/50], Step [221/531], Loss: 0.6149, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [4/50], Step [241/531], Loss: 0.7213, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [4/50], Step [261/531], Loss: 0.7464, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [4/50], Step [281/531], Loss: 0.8527, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.3529\n",
      "Epoch [4/50], Step [301/531], Loss: 0.6601, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [4/50], Step [321/531], Loss: 0.6457, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [4/50], Step [341/531], Loss: 0.5918, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [4/50], Step [361/531], Loss: 0.7281, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [4/50], Step [381/531], Loss: 0.7086, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [4/50], Step [401/531], Loss: 0.6893, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [4/50], Step [421/531], Loss: 0.6860, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.4286\n",
      "Epoch [4/50], Step [441/531], Loss: 0.7225, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [4/50], Step [461/531], Loss: 0.7876, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.3750\n",
      "Epoch [4/50], Step [481/531], Loss: 0.6695, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [4/50], Step [501/531], Loss: 0.6775, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4706\n",
      "Epoch [4/50], Step [521/531], Loss: 0.6567, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.6316\n",
      "Epoch [5/50], Step [1/531], Loss: 0.7283, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 6.0000, batch_f1_score: 0.6000\n",
      "Epoch [5/50], Step [21/531], Loss: 0.6783, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7619\n",
      "Epoch [5/50], Step [41/531], Loss: 0.6617, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [5/50], Step [61/531], Loss: 0.7087, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [81/531], Loss: 0.7175, tp_sum: 8.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6957\n",
      "Epoch [5/50], Step [101/531], Loss: 0.6308, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [121/531], Loss: 0.6274, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [5/50], Step [141/531], Loss: 0.5844, tp_sum: 10.0000, fp_sum: 0.0000, fn_sum: 3.0000, batch_f1_score: 0.8696\n",
      "Epoch [5/50], Step [161/531], Loss: 0.6992, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [181/531], Loss: 0.6993, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [5/50], Step [201/531], Loss: 0.6849, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [5/50], Step [221/531], Loss: 0.8050, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 5.0000, batch_f1_score: 0.3333\n",
      "Epoch [5/50], Step [241/531], Loss: 0.6376, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.5455\n",
      "Epoch [5/50], Step [261/531], Loss: 0.7325, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.2857\n",
      "Epoch [5/50], Step [281/531], Loss: 0.6892, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [5/50], Step [301/531], Loss: 0.5719, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [5/50], Step [321/531], Loss: 0.6989, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.7000\n",
      "Epoch [5/50], Step [341/531], Loss: 0.6117, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [5/50], Step [361/531], Loss: 0.6054, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [5/50], Step [381/531], Loss: 0.6021, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [401/531], Loss: 0.7009, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [5/50], Step [421/531], Loss: 0.6835, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [5/50], Step [441/531], Loss: 0.6859, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [461/531], Loss: 0.5965, tp_sum: 9.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8182\n",
      "Epoch [5/50], Step [481/531], Loss: 0.6576, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [5/50], Step [501/531], Loss: 0.6335, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [5/50], Step [521/531], Loss: 0.6569, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [6/50], Step [1/531], Loss: 0.6533, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [6/50], Step [21/531], Loss: 0.7406, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [6/50], Step [41/531], Loss: 0.7442, tp_sum: 8.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6957\n",
      "Epoch [6/50], Step [61/531], Loss: 0.6808, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.3750\n",
      "Epoch [6/50], Step [81/531], Loss: 0.5944, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [6/50], Step [101/531], Loss: 0.6626, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [6/50], Step [121/531], Loss: 0.7612, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.3750\n",
      "Epoch [6/50], Step [141/531], Loss: 0.5805, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [6/50], Step [161/531], Loss: 0.6646, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5333\n",
      "Epoch [6/50], Step [181/531], Loss: 0.6260, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [6/50], Step [201/531], Loss: 0.7097, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.4000\n",
      "Epoch [6/50], Step [221/531], Loss: 0.6235, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8421\n",
      "Epoch [6/50], Step [241/531], Loss: 0.5974, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [6/50], Step [261/531], Loss: 0.6798, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [6/50], Step [281/531], Loss: 0.6273, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [6/50], Step [301/531], Loss: 0.6522, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [6/50], Step [321/531], Loss: 0.7030, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [6/50], Step [341/531], Loss: 0.7228, tp_sum: 7.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [6/50], Step [361/531], Loss: 0.7187, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.3333\n",
      "Epoch [6/50], Step [381/531], Loss: 0.7118, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.4706\n",
      "Epoch [6/50], Step [401/531], Loss: 0.7393, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [6/50], Step [421/531], Loss: 0.6937, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [6/50], Step [441/531], Loss: 0.6194, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [6/50], Step [461/531], Loss: 0.5884, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [6/50], Step [481/531], Loss: 0.6987, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [6/50], Step [501/531], Loss: 0.6567, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [6/50], Step [521/531], Loss: 0.6001, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [7/50], Step [1/531], Loss: 0.7126, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.3750\n",
      "Epoch [7/50], Step [21/531], Loss: 0.6899, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [7/50], Step [41/531], Loss: 0.6771, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [7/50], Step [61/531], Loss: 0.6217, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.7000\n",
      "Epoch [7/50], Step [81/531], Loss: 0.7234, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.5882\n",
      "Epoch [7/50], Step [101/531], Loss: 0.7775, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.4211\n",
      "Epoch [7/50], Step [121/531], Loss: 0.6457, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [7/50], Step [141/531], Loss: 0.7154, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [7/50], Step [161/531], Loss: 0.7269, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [7/50], Step [181/531], Loss: 0.6863, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 7.0000, batch_f1_score: 0.5714\n",
      "Epoch [7/50], Step [201/531], Loss: 0.7381, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 7.0000, batch_f1_score: 0.4706\n",
      "Epoch [7/50], Step [221/531], Loss: 0.6248, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6667\n",
      "Epoch [7/50], Step [241/531], Loss: 0.6847, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [7/50], Step [261/531], Loss: 0.6739, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5263\n",
      "Epoch [7/50], Step [281/531], Loss: 0.8033, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 5.0000, batch_f1_score: 0.3529\n",
      "Epoch [7/50], Step [301/531], Loss: 0.5984, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.9000\n",
      "Epoch [7/50], Step [321/531], Loss: 0.7582, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [7/50], Step [341/531], Loss: 0.6681, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [7/50], Step [361/531], Loss: 0.6024, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [7/50], Step [381/531], Loss: 0.6612, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [7/50], Step [401/531], Loss: 0.6328, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [7/50], Step [421/531], Loss: 0.6005, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [7/50], Step [441/531], Loss: 0.6068, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [7/50], Step [461/531], Loss: 0.7202, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [7/50], Step [481/531], Loss: 0.7105, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [7/50], Step [501/531], Loss: 0.7237, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.6316\n",
      "Epoch [7/50], Step [521/531], Loss: 0.8107, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.3529\n",
      "Epoch [8/50], Step [1/531], Loss: 0.6194, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [8/50], Step [21/531], Loss: 0.7301, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [8/50], Step [41/531], Loss: 0.6720, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [8/50], Step [61/531], Loss: 0.6224, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5882\n",
      "Epoch [8/50], Step [81/531], Loss: 0.7039, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [8/50], Step [101/531], Loss: 0.6089, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [8/50], Step [121/531], Loss: 0.7033, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [8/50], Step [141/531], Loss: 0.6490, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [8/50], Step [161/531], Loss: 0.7640, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [8/50], Step [181/531], Loss: 0.6540, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [8/50], Step [201/531], Loss: 0.7570, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5714\n",
      "Epoch [8/50], Step [221/531], Loss: 0.6336, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8000\n",
      "Epoch [8/50], Step [241/531], Loss: 0.6649, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [8/50], Step [261/531], Loss: 0.7104, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5263\n",
      "Epoch [8/50], Step [281/531], Loss: 0.6698, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7368\n",
      "Epoch [8/50], Step [301/531], Loss: 0.6972, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6957\n",
      "Epoch [8/50], Step [321/531], Loss: 0.6944, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [8/50], Step [341/531], Loss: 0.6618, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [8/50], Step [361/531], Loss: 0.6377, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [8/50], Step [381/531], Loss: 0.6701, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [8/50], Step [401/531], Loss: 0.6976, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [8/50], Step [421/531], Loss: 0.7063, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [8/50], Step [441/531], Loss: 0.6632, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [8/50], Step [461/531], Loss: 0.5902, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7143\n",
      "Epoch [8/50], Step [481/531], Loss: 0.5964, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [8/50], Step [501/531], Loss: 0.6725, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [8/50], Step [521/531], Loss: 0.7248, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [9/50], Step [1/531], Loss: 0.6662, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [21/531], Loss: 0.6302, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [9/50], Step [41/531], Loss: 0.6701, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.4286\n",
      "Epoch [9/50], Step [61/531], Loss: 0.6933, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [9/50], Step [81/531], Loss: 0.6635, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [9/50], Step [101/531], Loss: 0.6514, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [121/531], Loss: 0.7505, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.3750\n",
      "Epoch [9/50], Step [141/531], Loss: 0.6364, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [9/50], Step [161/531], Loss: 0.5684, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [9/50], Step [181/531], Loss: 0.8024, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 7.0000, batch_f1_score: 0.4211\n",
      "Epoch [9/50], Step [201/531], Loss: 0.6179, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [9/50], Step [221/531], Loss: 0.6456, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [9/50], Step [241/531], Loss: 0.6771, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [9/50], Step [261/531], Loss: 0.6814, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [9/50], Step [281/531], Loss: 0.7130, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [9/50], Step [301/531], Loss: 0.6681, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n",
      "Epoch [9/50], Step [321/531], Loss: 0.6713, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [9/50], Step [341/531], Loss: 0.6363, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5882\n",
      "Epoch [9/50], Step [361/531], Loss: 0.7052, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [381/531], Loss: 0.6844, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [9/50], Step [401/531], Loss: 0.5986, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [421/531], Loss: 0.6500, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [9/50], Step [441/531], Loss: 0.6554, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [9/50], Step [461/531], Loss: 0.6566, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [9/50], Step [481/531], Loss: 0.7354, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [9/50], Step [501/531], Loss: 0.6525, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [9/50], Step [521/531], Loss: 0.6163, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [10/50], Step [1/531], Loss: 0.6282, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [10/50], Step [21/531], Loss: 0.7060, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [10/50], Step [41/531], Loss: 0.6497, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [10/50], Step [61/531], Loss: 0.7004, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [10/50], Step [81/531], Loss: 0.7255, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [10/50], Step [101/531], Loss: 0.6703, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [10/50], Step [121/531], Loss: 0.6405, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.7059\n",
      "Epoch [10/50], Step [141/531], Loss: 0.7337, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [10/50], Step [161/531], Loss: 0.6101, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [10/50], Step [181/531], Loss: 0.6053, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [10/50], Step [201/531], Loss: 0.6749, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [10/50], Step [221/531], Loss: 0.6698, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [10/50], Step [241/531], Loss: 0.7136, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [10/50], Step [261/531], Loss: 0.7344, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 6.0000, batch_f1_score: 0.6364\n",
      "Epoch [10/50], Step [281/531], Loss: 0.7019, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [10/50], Step [301/531], Loss: 0.6351, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [10/50], Step [321/531], Loss: 0.6235, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [10/50], Step [341/531], Loss: 0.7048, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [10/50], Step [361/531], Loss: 0.7409, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [10/50], Step [381/531], Loss: 0.5529, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [10/50], Step [401/531], Loss: 0.6104, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [10/50], Step [421/531], Loss: 0.6990, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [10/50], Step [441/531], Loss: 0.6712, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.3636\n",
      "Epoch [10/50], Step [461/531], Loss: 0.6124, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [10/50], Step [481/531], Loss: 0.7889, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 5.0000, batch_f1_score: 0.2353\n",
      "Epoch [10/50], Step [501/531], Loss: 0.6713, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [10/50], Step [521/531], Loss: 0.5385, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [11/50], Step [1/531], Loss: 0.7429, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [11/50], Step [21/531], Loss: 0.7667, tp_sum: 1.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.1538\n",
      "Epoch [11/50], Step [41/531], Loss: 0.7262, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5714\n",
      "Epoch [11/50], Step [61/531], Loss: 0.7513, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.2857\n",
      "Epoch [11/50], Step [81/531], Loss: 0.6426, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [11/50], Step [101/531], Loss: 0.6049, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.9000\n",
      "Epoch [11/50], Step [121/531], Loss: 0.8137, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.3529\n",
      "Epoch [11/50], Step [141/531], Loss: 0.7132, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [11/50], Step [161/531], Loss: 0.6092, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8235\n",
      "Epoch [11/50], Step [181/531], Loss: 0.6377, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [11/50], Step [201/531], Loss: 0.7010, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 2.0000, batch_f1_score: 0.2857\n",
      "Epoch [11/50], Step [221/531], Loss: 0.6103, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [11/50], Step [241/531], Loss: 0.6615, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [11/50], Step [261/531], Loss: 0.6211, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [11/50], Step [281/531], Loss: 0.7903, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 5.0000, batch_f1_score: 0.2500\n",
      "Epoch [11/50], Step [301/531], Loss: 0.7827, tp_sum: 1.0000, fp_sum: 4.0000, fn_sum: 8.0000, batch_f1_score: 0.1429\n",
      "Epoch [11/50], Step [321/531], Loss: 0.6342, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [11/50], Step [341/531], Loss: 0.7014, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [11/50], Step [361/531], Loss: 0.6576, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [11/50], Step [381/531], Loss: 0.7125, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 1.0000, batch_f1_score: 0.3077\n",
      "Epoch [11/50], Step [401/531], Loss: 0.6001, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [11/50], Step [421/531], Loss: 0.6346, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8421\n",
      "Epoch [11/50], Step [441/531], Loss: 0.7318, tp_sum: 3.0000, fp_sum: 2.0000, fn_sum: 7.0000, batch_f1_score: 0.4000\n",
      "Epoch [11/50], Step [461/531], Loss: 0.6976, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [11/50], Step [481/531], Loss: 0.7095, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [11/50], Step [501/531], Loss: 0.6484, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [11/50], Step [521/531], Loss: 0.6624, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [12/50], Step [1/531], Loss: 0.6905, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [12/50], Step [21/531], Loss: 0.6813, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4706\n",
      "Epoch [12/50], Step [41/531], Loss: 0.6538, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.6667\n",
      "Epoch [12/50], Step [61/531], Loss: 0.6335, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [12/50], Step [81/531], Loss: 0.6650, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [12/50], Step [101/531], Loss: 0.6692, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [12/50], Step [121/531], Loss: 0.6473, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [12/50], Step [141/531], Loss: 0.6560, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [12/50], Step [161/531], Loss: 0.6075, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7778\n",
      "Epoch [12/50], Step [181/531], Loss: 0.5926, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [12/50], Step [201/531], Loss: 0.6183, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [12/50], Step [221/531], Loss: 0.7294, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [12/50], Step [241/531], Loss: 0.6638, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [12/50], Step [261/531], Loss: 0.6011, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [12/50], Step [281/531], Loss: 0.6384, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [12/50], Step [301/531], Loss: 0.7010, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [12/50], Step [321/531], Loss: 0.6884, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n",
      "Epoch [12/50], Step [341/531], Loss: 0.7290, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.6000\n",
      "Epoch [12/50], Step [361/531], Loss: 0.6902, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5263\n",
      "Epoch [12/50], Step [381/531], Loss: 0.7385, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.4444\n",
      "Epoch [12/50], Step [401/531], Loss: 0.8080, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4706\n",
      "Epoch [12/50], Step [421/531], Loss: 0.6239, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8571\n",
      "Epoch [12/50], Step [441/531], Loss: 0.6804, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [12/50], Step [461/531], Loss: 0.5662, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [12/50], Step [481/531], Loss: 0.6003, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [12/50], Step [501/531], Loss: 0.7154, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [12/50], Step [521/531], Loss: 0.7242, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [13/50], Step [1/531], Loss: 0.7324, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5333\n",
      "Epoch [13/50], Step [21/531], Loss: 0.6457, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.5882\n",
      "Epoch [13/50], Step [41/531], Loss: 0.6073, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [13/50], Step [61/531], Loss: 0.7239, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [13/50], Step [81/531], Loss: 0.6375, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [13/50], Step [101/531], Loss: 0.7660, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [13/50], Step [121/531], Loss: 0.6182, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [13/50], Step [141/531], Loss: 0.6678, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 0.0000, batch_f1_score: 0.4615\n",
      "Epoch [13/50], Step [161/531], Loss: 0.6830, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [13/50], Step [181/531], Loss: 0.7001, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.4000\n",
      "Epoch [13/50], Step [201/531], Loss: 0.6299, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [13/50], Step [221/531], Loss: 0.6553, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [13/50], Step [241/531], Loss: 0.6783, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [13/50], Step [261/531], Loss: 0.6782, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [13/50], Step [281/531], Loss: 0.7056, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [13/50], Step [301/531], Loss: 0.7148, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [13/50], Step [321/531], Loss: 0.7819, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.3750\n",
      "Epoch [13/50], Step [341/531], Loss: 0.7362, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [13/50], Step [361/531], Loss: 0.8254, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 5.0000, batch_f1_score: 0.3529\n",
      "Epoch [13/50], Step [381/531], Loss: 0.5683, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 0.0000, batch_f1_score: 0.8750\n",
      "Epoch [13/50], Step [401/531], Loss: 0.6718, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [13/50], Step [421/531], Loss: 0.8143, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.5455\n",
      "Epoch [13/50], Step [441/531], Loss: 0.6902, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [13/50], Step [461/531], Loss: 0.6641, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [13/50], Step [481/531], Loss: 0.5933, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [13/50], Step [501/531], Loss: 0.6242, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [13/50], Step [521/531], Loss: 0.7556, tp_sum: 1.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.1667\n",
      "Epoch [14/50], Step [1/531], Loss: 0.6016, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [14/50], Step [21/531], Loss: 0.6955, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [14/50], Step [41/531], Loss: 0.7790, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.4444\n",
      "Epoch [14/50], Step [61/531], Loss: 0.6591, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [14/50], Step [81/531], Loss: 0.6807, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [14/50], Step [101/531], Loss: 0.6798, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.3333\n",
      "Epoch [14/50], Step [121/531], Loss: 0.7284, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5714\n",
      "Epoch [14/50], Step [141/531], Loss: 0.7236, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [14/50], Step [161/531], Loss: 0.6418, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7143\n",
      "Epoch [14/50], Step [181/531], Loss: 0.6914, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [14/50], Step [201/531], Loss: 0.6723, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [14/50], Step [221/531], Loss: 0.6343, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [14/50], Step [241/531], Loss: 0.5662, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [14/50], Step [261/531], Loss: 0.7306, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [14/50], Step [281/531], Loss: 0.6447, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [14/50], Step [301/531], Loss: 0.7756, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5263\n",
      "Epoch [14/50], Step [321/531], Loss: 0.6421, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [14/50], Step [341/531], Loss: 0.6878, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [14/50], Step [361/531], Loss: 0.6151, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [14/50], Step [381/531], Loss: 0.7018, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5714\n",
      "Epoch [14/50], Step [401/531], Loss: 0.7671, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.4444\n",
      "Epoch [14/50], Step [421/531], Loss: 0.6648, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [14/50], Step [441/531], Loss: 0.6184, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [14/50], Step [461/531], Loss: 0.7327, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [14/50], Step [481/531], Loss: 0.6397, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [14/50], Step [501/531], Loss: 0.7081, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [14/50], Step [521/531], Loss: 0.7107, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [15/50], Step [1/531], Loss: 0.7491, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.4286\n",
      "Epoch [15/50], Step [21/531], Loss: 0.7292, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5263\n",
      "Epoch [15/50], Step [41/531], Loss: 0.6153, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [15/50], Step [61/531], Loss: 0.7241, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [15/50], Step [81/531], Loss: 0.6610, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5000\n",
      "Epoch [15/50], Step [101/531], Loss: 0.7629, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.3750\n",
      "Epoch [15/50], Step [121/531], Loss: 0.6776, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.4706\n",
      "Epoch [15/50], Step [141/531], Loss: 0.7785, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [15/50], Step [161/531], Loss: 0.5951, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [15/50], Step [181/531], Loss: 0.6748, tp_sum: 10.0000, fp_sum: 0.0000, fn_sum: 5.0000, batch_f1_score: 0.8000\n",
      "Epoch [15/50], Step [201/531], Loss: 0.6013, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7500\n",
      "Epoch [15/50], Step [221/531], Loss: 0.6173, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.7059\n",
      "Epoch [15/50], Step [241/531], Loss: 0.7071, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [15/50], Step [261/531], Loss: 0.6837, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [15/50], Step [281/531], Loss: 0.6881, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [15/50], Step [301/531], Loss: 0.6964, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [15/50], Step [321/531], Loss: 0.6867, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [15/50], Step [341/531], Loss: 0.6217, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [15/50], Step [361/531], Loss: 0.6621, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [15/50], Step [381/531], Loss: 0.5653, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8571\n",
      "Epoch [15/50], Step [401/531], Loss: 0.7049, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [15/50], Step [421/531], Loss: 0.6649, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [15/50], Step [441/531], Loss: 0.7120, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.3077\n",
      "Epoch [15/50], Step [461/531], Loss: 0.7120, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [15/50], Step [481/531], Loss: 0.6617, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [15/50], Step [501/531], Loss: 0.6692, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.4286\n",
      "Epoch [15/50], Step [521/531], Loss: 0.6128, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5714\n",
      "Epoch [16/50], Step [1/531], Loss: 0.6881, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [16/50], Step [21/531], Loss: 0.6883, tp_sum: 5.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.5263\n",
      "Epoch [16/50], Step [41/531], Loss: 0.6584, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [16/50], Step [61/531], Loss: 0.7474, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 6.0000, batch_f1_score: 0.2500\n",
      "Epoch [16/50], Step [81/531], Loss: 0.6560, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [16/50], Step [101/531], Loss: 0.6418, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [16/50], Step [121/531], Loss: 0.6179, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7619\n",
      "Epoch [16/50], Step [141/531], Loss: 0.6269, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [16/50], Step [161/531], Loss: 0.6247, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [16/50], Step [181/531], Loss: 0.7051, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4706\n",
      "Epoch [16/50], Step [201/531], Loss: 0.6219, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [16/50], Step [221/531], Loss: 0.6033, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [16/50], Step [241/531], Loss: 0.6109, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [16/50], Step [261/531], Loss: 0.6745, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [16/50], Step [281/531], Loss: 0.6849, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [16/50], Step [301/531], Loss: 0.6873, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [16/50], Step [321/531], Loss: 0.6599, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [16/50], Step [341/531], Loss: 0.6562, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [16/50], Step [361/531], Loss: 0.7326, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [16/50], Step [381/531], Loss: 0.6491, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [16/50], Step [401/531], Loss: 0.6179, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [16/50], Step [421/531], Loss: 0.5911, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7619\n",
      "Epoch [16/50], Step [441/531], Loss: 0.6894, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [16/50], Step [461/531], Loss: 0.6018, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [16/50], Step [481/531], Loss: 0.6316, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [16/50], Step [501/531], Loss: 0.6550, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5714\n",
      "Epoch [16/50], Step [521/531], Loss: 0.6284, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [17/50], Step [1/531], Loss: 0.6710, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [17/50], Step [21/531], Loss: 0.7005, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [17/50], Step [41/531], Loss: 0.6494, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [17/50], Step [61/531], Loss: 0.6637, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [17/50], Step [81/531], Loss: 0.6082, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [17/50], Step [101/531], Loss: 0.7310, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [17/50], Step [121/531], Loss: 0.7398, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.3077\n",
      "Epoch [17/50], Step [141/531], Loss: 0.6936, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [17/50], Step [161/531], Loss: 0.6092, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8421\n",
      "Epoch [17/50], Step [181/531], Loss: 0.7109, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [17/50], Step [201/531], Loss: 0.6785, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [17/50], Step [221/531], Loss: 0.6294, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 0.0000, batch_f1_score: 0.8889\n",
      "Epoch [17/50], Step [241/531], Loss: 0.6898, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 6.0000, batch_f1_score: 0.6000\n",
      "Epoch [17/50], Step [261/531], Loss: 0.7513, tp_sum: 6.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.6000\n",
      "Epoch [17/50], Step [281/531], Loss: 0.7212, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [17/50], Step [301/531], Loss: 0.7166, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 7.0000, batch_f1_score: 0.6087\n",
      "Epoch [17/50], Step [321/531], Loss: 0.7691, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.4615\n",
      "Epoch [17/50], Step [341/531], Loss: 0.6960, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [17/50], Step [361/531], Loss: 0.6320, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6667\n",
      "Epoch [17/50], Step [381/531], Loss: 0.6322, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [17/50], Step [401/531], Loss: 0.6670, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [17/50], Step [421/531], Loss: 0.5858, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [17/50], Step [441/531], Loss: 0.6246, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [17/50], Step [461/531], Loss: 0.7352, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [17/50], Step [481/531], Loss: 0.6956, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [17/50], Step [501/531], Loss: 0.7046, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [17/50], Step [521/531], Loss: 0.6028, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [18/50], Step [1/531], Loss: 0.6288, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [18/50], Step [21/531], Loss: 0.8142, tp_sum: 5.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.4762\n",
      "Epoch [18/50], Step [41/531], Loss: 0.6649, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.3333\n",
      "Epoch [18/50], Step [61/531], Loss: 0.6633, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [18/50], Step [81/531], Loss: 0.6550, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.7000\n",
      "Epoch [18/50], Step [101/531], Loss: 0.6072, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.7273\n",
      "Epoch [18/50], Step [121/531], Loss: 0.6812, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [18/50], Step [141/531], Loss: 0.6844, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [18/50], Step [161/531], Loss: 0.6283, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8000\n",
      "Epoch [18/50], Step [181/531], Loss: 0.6614, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [18/50], Step [201/531], Loss: 0.7647, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [18/50], Step [221/531], Loss: 0.6803, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.6000\n",
      "Epoch [18/50], Step [241/531], Loss: 0.6421, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [18/50], Step [261/531], Loss: 0.6792, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5263\n",
      "Epoch [18/50], Step [281/531], Loss: 0.5671, tp_sum: 9.0000, fp_sum: 3.0000, fn_sum: 0.0000, batch_f1_score: 0.8571\n",
      "Epoch [18/50], Step [301/531], Loss: 0.6471, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7368\n",
      "Epoch [18/50], Step [321/531], Loss: 0.5867, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 0.0000, batch_f1_score: 0.8421\n",
      "Epoch [18/50], Step [341/531], Loss: 0.6388, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [18/50], Step [361/531], Loss: 0.6611, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [18/50], Step [381/531], Loss: 0.7544, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [18/50], Step [401/531], Loss: 0.7142, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [18/50], Step [421/531], Loss: 0.6968, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [18/50], Step [441/531], Loss: 0.7384, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [18/50], Step [461/531], Loss: 0.6418, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [18/50], Step [481/531], Loss: 0.5865, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7619\n",
      "Epoch [18/50], Step [501/531], Loss: 0.6639, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [18/50], Step [521/531], Loss: 0.5796, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8000\n",
      "Epoch [19/50], Step [1/531], Loss: 0.7321, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5714\n",
      "Epoch [19/50], Step [21/531], Loss: 0.7819, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [19/50], Step [41/531], Loss: 0.6877, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [19/50], Step [61/531], Loss: 0.6122, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [19/50], Step [81/531], Loss: 0.7063, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7273\n",
      "Epoch [19/50], Step [101/531], Loss: 0.6601, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [19/50], Step [121/531], Loss: 0.6349, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [19/50], Step [141/531], Loss: 0.7868, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.4444\n",
      "Epoch [19/50], Step [161/531], Loss: 0.7527, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 7.0000, batch_f1_score: 0.3529\n",
      "Epoch [19/50], Step [181/531], Loss: 0.7078, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [19/50], Step [201/531], Loss: 0.7403, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [19/50], Step [221/531], Loss: 0.6842, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [19/50], Step [241/531], Loss: 0.7154, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [19/50], Step [261/531], Loss: 0.6568, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [19/50], Step [281/531], Loss: 0.6238, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7500\n",
      "Epoch [19/50], Step [301/531], Loss: 0.6894, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [19/50], Step [321/531], Loss: 0.6957, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 7.0000, batch_f1_score: 0.5263\n",
      "Epoch [19/50], Step [341/531], Loss: 0.6636, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [19/50], Step [361/531], Loss: 0.6185, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7619\n",
      "Epoch [19/50], Step [381/531], Loss: 0.6719, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [19/50], Step [401/531], Loss: 0.6657, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [19/50], Step [421/531], Loss: 0.7106, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [19/50], Step [441/531], Loss: 0.6928, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [19/50], Step [461/531], Loss: 0.6665, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [19/50], Step [481/531], Loss: 0.6827, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [19/50], Step [501/531], Loss: 0.6732, tp_sum: 6.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.6316\n",
      "Epoch [19/50], Step [521/531], Loss: 0.6929, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [20/50], Step [1/531], Loss: 0.6376, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.4615\n",
      "Epoch [20/50], Step [21/531], Loss: 0.7649, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [20/50], Step [41/531], Loss: 0.6639, tp_sum: 8.0000, fp_sum: 0.0000, fn_sum: 7.0000, batch_f1_score: 0.6957\n",
      "Epoch [20/50], Step [61/531], Loss: 0.6360, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [20/50], Step [81/531], Loss: 0.7023, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 7.0000, batch_f1_score: 0.3750\n",
      "Epoch [20/50], Step [101/531], Loss: 0.6693, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [20/50], Step [121/531], Loss: 0.7463, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.5455\n",
      "Epoch [20/50], Step [141/531], Loss: 0.6214, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [20/50], Step [161/531], Loss: 0.6916, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [20/50], Step [181/531], Loss: 0.7380, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.3529\n",
      "Epoch [20/50], Step [201/531], Loss: 0.6496, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [20/50], Step [221/531], Loss: 0.7643, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.5000\n",
      "Epoch [20/50], Step [241/531], Loss: 0.6097, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [20/50], Step [261/531], Loss: 0.6631, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.4000\n",
      "Epoch [20/50], Step [281/531], Loss: 0.6245, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [20/50], Step [301/531], Loss: 0.7066, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [20/50], Step [321/531], Loss: 0.6747, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [20/50], Step [341/531], Loss: 0.6224, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [20/50], Step [361/531], Loss: 0.7551, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [20/50], Step [381/531], Loss: 0.5828, tp_sum: 10.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.9091\n",
      "Epoch [20/50], Step [401/531], Loss: 0.7194, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [20/50], Step [421/531], Loss: 0.6272, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [20/50], Step [441/531], Loss: 0.6932, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [20/50], Step [461/531], Loss: 0.7292, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.2857\n",
      "Epoch [20/50], Step [481/531], Loss: 0.6755, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5263\n",
      "Epoch [20/50], Step [501/531], Loss: 0.7650, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.5000\n",
      "Epoch [20/50], Step [521/531], Loss: 0.5992, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 0.0000, batch_f1_score: 0.8421\n",
      "Epoch [21/50], Step [1/531], Loss: 0.6751, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5263\n",
      "Epoch [21/50], Step [21/531], Loss: 0.6614, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [21/50], Step [41/531], Loss: 0.7108, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [21/50], Step [61/531], Loss: 0.6379, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [21/50], Step [81/531], Loss: 0.6606, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [21/50], Step [101/531], Loss: 0.7931, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.3750\n",
      "Epoch [21/50], Step [121/531], Loss: 0.6345, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [21/50], Step [141/531], Loss: 0.6691, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [21/50], Step [161/531], Loss: 0.6740, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.6316\n",
      "Epoch [21/50], Step [181/531], Loss: 0.6750, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [21/50], Step [201/531], Loss: 0.6909, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.3333\n",
      "Epoch [21/50], Step [221/531], Loss: 0.7267, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [21/50], Step [241/531], Loss: 0.6658, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [21/50], Step [261/531], Loss: 0.6715, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [21/50], Step [281/531], Loss: 0.6815, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [21/50], Step [301/531], Loss: 0.6081, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [21/50], Step [321/531], Loss: 0.6433, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [21/50], Step [341/531], Loss: 0.6943, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [21/50], Step [361/531], Loss: 0.6667, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [21/50], Step [381/531], Loss: 0.5868, tp_sum: 8.0000, fp_sum: 0.0000, fn_sum: 2.0000, batch_f1_score: 0.8889\n",
      "Epoch [21/50], Step [401/531], Loss: 0.6152, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [21/50], Step [421/531], Loss: 0.6615, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [21/50], Step [441/531], Loss: 0.6388, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [21/50], Step [461/531], Loss: 0.5990, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [21/50], Step [481/531], Loss: 0.6612, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [21/50], Step [501/531], Loss: 0.5705, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [21/50], Step [521/531], Loss: 0.7208, tp_sum: 8.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.7273\n",
      "Epoch [22/50], Step [1/531], Loss: 0.6526, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [22/50], Step [21/531], Loss: 0.6096, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [22/50], Step [41/531], Loss: 0.6761, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [22/50], Step [61/531], Loss: 0.6062, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [22/50], Step [81/531], Loss: 0.6551, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [22/50], Step [101/531], Loss: 0.6846, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [22/50], Step [121/531], Loss: 0.5984, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 0.0000, batch_f1_score: 0.6250\n",
      "Epoch [22/50], Step [141/531], Loss: 0.7541, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [22/50], Step [161/531], Loss: 0.5959, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [22/50], Step [181/531], Loss: 0.6788, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [22/50], Step [201/531], Loss: 0.6959, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [22/50], Step [221/531], Loss: 0.6089, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7778\n",
      "Epoch [22/50], Step [241/531], Loss: 0.5851, tp_sum: 7.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.7368\n",
      "Epoch [22/50], Step [261/531], Loss: 0.6861, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7778\n",
      "Epoch [22/50], Step [281/531], Loss: 0.6611, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [22/50], Step [301/531], Loss: 0.7088, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [22/50], Step [321/531], Loss: 0.6317, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6667\n",
      "Epoch [22/50], Step [341/531], Loss: 0.6807, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [22/50], Step [361/531], Loss: 0.6212, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [22/50], Step [381/531], Loss: 0.6507, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [22/50], Step [401/531], Loss: 0.6935, tp_sum: 3.0000, fp_sum: 2.0000, fn_sum: 8.0000, batch_f1_score: 0.3750\n",
      "Epoch [22/50], Step [421/531], Loss: 0.6290, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [22/50], Step [441/531], Loss: 0.6176, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.7059\n",
      "Epoch [22/50], Step [461/531], Loss: 0.6094, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [22/50], Step [481/531], Loss: 0.6560, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [22/50], Step [501/531], Loss: 0.6365, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7059\n",
      "Epoch [22/50], Step [521/531], Loss: 0.7544, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [23/50], Step [1/531], Loss: 0.7064, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [23/50], Step [21/531], Loss: 0.6458, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [23/50], Step [41/531], Loss: 0.7321, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [23/50], Step [61/531], Loss: 0.6232, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.7273\n",
      "Epoch [23/50], Step [81/531], Loss: 0.6502, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [23/50], Step [101/531], Loss: 0.7384, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [23/50], Step [121/531], Loss: 0.6086, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [23/50], Step [141/531], Loss: 0.8531, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.3529\n",
      "Epoch [23/50], Step [161/531], Loss: 0.6061, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.9000\n",
      "Epoch [23/50], Step [181/531], Loss: 0.6054, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.7000\n",
      "Epoch [23/50], Step [201/531], Loss: 0.6440, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [23/50], Step [221/531], Loss: 0.6369, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [23/50], Step [241/531], Loss: 0.6495, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [23/50], Step [261/531], Loss: 0.6429, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [23/50], Step [281/531], Loss: 0.6909, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [23/50], Step [301/531], Loss: 0.6615, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [23/50], Step [321/531], Loss: 0.6220, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [23/50], Step [341/531], Loss: 0.6561, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8571\n",
      "Epoch [23/50], Step [361/531], Loss: 0.5921, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [23/50], Step [381/531], Loss: 0.7563, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [23/50], Step [401/531], Loss: 0.5660, tp_sum: 9.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8571\n",
      "Epoch [23/50], Step [421/531], Loss: 0.6695, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [23/50], Step [441/531], Loss: 0.7704, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 5.0000, batch_f1_score: 0.4211\n",
      "Epoch [23/50], Step [461/531], Loss: 0.7004, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [23/50], Step [481/531], Loss: 0.6529, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [23/50], Step [501/531], Loss: 0.6316, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7059\n",
      "Epoch [23/50], Step [521/531], Loss: 0.7123, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [24/50], Step [1/531], Loss: 0.6174, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7500\n",
      "Epoch [24/50], Step [21/531], Loss: 0.7365, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [24/50], Step [41/531], Loss: 0.6966, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [24/50], Step [61/531], Loss: 0.6655, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [24/50], Step [81/531], Loss: 0.6400, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5882\n",
      "Epoch [24/50], Step [101/531], Loss: 0.6799, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [24/50], Step [121/531], Loss: 0.6516, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [24/50], Step [141/531], Loss: 0.7759, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 4.0000, batch_f1_score: 0.2500\n",
      "Epoch [24/50], Step [161/531], Loss: 0.6531, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [24/50], Step [181/531], Loss: 0.6362, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [24/50], Step [201/531], Loss: 0.6673, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [24/50], Step [221/531], Loss: 0.6196, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7143\n",
      "Epoch [24/50], Step [241/531], Loss: 0.7295, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.5000\n",
      "Epoch [24/50], Step [261/531], Loss: 0.6831, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [24/50], Step [281/531], Loss: 0.6768, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [24/50], Step [301/531], Loss: 0.6934, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [24/50], Step [321/531], Loss: 0.6498, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7778\n",
      "Epoch [24/50], Step [341/531], Loss: 0.7461, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [24/50], Step [361/531], Loss: 0.6232, tp_sum: 10.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8333\n",
      "Epoch [24/50], Step [381/531], Loss: 0.6916, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [24/50], Step [401/531], Loss: 0.6762, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [24/50], Step [421/531], Loss: 0.6965, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [24/50], Step [441/531], Loss: 0.6693, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [24/50], Step [461/531], Loss: 0.6768, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [24/50], Step [481/531], Loss: 0.6706, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [24/50], Step [501/531], Loss: 0.6405, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [24/50], Step [521/531], Loss: 0.7142, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [25/50], Step [1/531], Loss: 0.6980, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [25/50], Step [21/531], Loss: 0.6584, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [25/50], Step [41/531], Loss: 0.6549, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [25/50], Step [61/531], Loss: 0.6556, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [25/50], Step [81/531], Loss: 0.7022, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [25/50], Step [101/531], Loss: 0.6787, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [25/50], Step [121/531], Loss: 0.6719, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [25/50], Step [141/531], Loss: 0.7445, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [25/50], Step [161/531], Loss: 0.6726, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.7000\n",
      "Epoch [25/50], Step [181/531], Loss: 0.5935, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [25/50], Step [201/531], Loss: 0.6637, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [25/50], Step [221/531], Loss: 0.6915, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [25/50], Step [241/531], Loss: 0.6838, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [25/50], Step [261/531], Loss: 0.6456, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [25/50], Step [281/531], Loss: 0.6068, tp_sum: 9.0000, fp_sum: 0.0000, fn_sum: 3.0000, batch_f1_score: 0.8571\n",
      "Epoch [25/50], Step [301/531], Loss: 0.5864, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [25/50], Step [321/531], Loss: 0.5792, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [25/50], Step [341/531], Loss: 0.5825, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [25/50], Step [361/531], Loss: 0.6566, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [25/50], Step [381/531], Loss: 0.6264, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [25/50], Step [401/531], Loss: 0.7092, tp_sum: 7.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [25/50], Step [421/531], Loss: 0.6622, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [25/50], Step [441/531], Loss: 0.5978, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [25/50], Step [461/531], Loss: 0.7232, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [25/50], Step [481/531], Loss: 0.7242, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 2.0000, batch_f1_score: 0.2857\n",
      "Epoch [25/50], Step [501/531], Loss: 0.6733, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [25/50], Step [521/531], Loss: 0.7671, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [26/50], Step [1/531], Loss: 0.6485, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [26/50], Step [21/531], Loss: 0.6381, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [26/50], Step [41/531], Loss: 0.6666, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [26/50], Step [61/531], Loss: 0.6189, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [26/50], Step [81/531], Loss: 0.5976, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 0.0000, batch_f1_score: 0.8750\n",
      "Epoch [26/50], Step [101/531], Loss: 0.7392, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.6316\n",
      "Epoch [26/50], Step [121/531], Loss: 0.6687, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [26/50], Step [141/531], Loss: 0.6447, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.6667\n",
      "Epoch [26/50], Step [161/531], Loss: 0.5872, tp_sum: 6.0000, fp_sum: 0.0000, fn_sum: 3.0000, batch_f1_score: 0.8000\n",
      "Epoch [26/50], Step [181/531], Loss: 0.6946, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.4286\n",
      "Epoch [26/50], Step [201/531], Loss: 0.6960, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [26/50], Step [221/531], Loss: 0.6993, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [26/50], Step [241/531], Loss: 0.7132, tp_sum: 1.0000, fp_sum: 8.0000, fn_sum: 1.0000, batch_f1_score: 0.1818\n",
      "Epoch [26/50], Step [261/531], Loss: 0.5913, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [26/50], Step [281/531], Loss: 0.6703, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [26/50], Step [301/531], Loss: 0.7091, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [26/50], Step [321/531], Loss: 0.6173, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [26/50], Step [341/531], Loss: 0.6994, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [26/50], Step [361/531], Loss: 0.6673, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [26/50], Step [381/531], Loss: 0.6950, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [26/50], Step [401/531], Loss: 0.6965, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [26/50], Step [421/531], Loss: 0.6887, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [26/50], Step [441/531], Loss: 0.6060, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [26/50], Step [461/531], Loss: 0.6344, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [26/50], Step [481/531], Loss: 0.6755, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5333\n",
      "Epoch [26/50], Step [501/531], Loss: 0.6998, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [26/50], Step [521/531], Loss: 0.7317, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [27/50], Step [1/531], Loss: 0.6606, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [27/50], Step [21/531], Loss: 0.5829, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [27/50], Step [41/531], Loss: 0.5772, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 0.0000, batch_f1_score: 0.8421\n",
      "Epoch [27/50], Step [61/531], Loss: 0.7109, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [27/50], Step [81/531], Loss: 0.7010, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [27/50], Step [101/531], Loss: 0.7245, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5714\n",
      "Epoch [27/50], Step [121/531], Loss: 0.7286, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [27/50], Step [141/531], Loss: 0.7234, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [27/50], Step [161/531], Loss: 0.5539, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 0.0000, batch_f1_score: 0.9474\n",
      "Epoch [27/50], Step [181/531], Loss: 0.7383, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [27/50], Step [201/531], Loss: 0.6604, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [27/50], Step [221/531], Loss: 0.6974, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.3333\n",
      "Epoch [27/50], Step [241/531], Loss: 0.6629, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [27/50], Step [261/531], Loss: 0.6802, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [27/50], Step [281/531], Loss: 0.8183, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.2667\n",
      "Epoch [27/50], Step [301/531], Loss: 0.6605, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [27/50], Step [321/531], Loss: 0.5996, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [27/50], Step [341/531], Loss: 0.6165, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [27/50], Step [361/531], Loss: 0.7071, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [27/50], Step [381/531], Loss: 0.6771, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [27/50], Step [401/531], Loss: 0.6882, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [27/50], Step [421/531], Loss: 0.6294, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7778\n",
      "Epoch [27/50], Step [441/531], Loss: 0.6779, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [27/50], Step [461/531], Loss: 0.6645, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5333\n",
      "Epoch [27/50], Step [481/531], Loss: 0.6289, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [27/50], Step [501/531], Loss: 0.6371, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [27/50], Step [521/531], Loss: 0.6300, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.5556\n",
      "Epoch [28/50], Step [1/531], Loss: 0.6694, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [28/50], Step [21/531], Loss: 0.7137, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [28/50], Step [41/531], Loss: 0.7687, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [28/50], Step [61/531], Loss: 0.6671, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [28/50], Step [81/531], Loss: 0.7431, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 3.0000, batch_f1_score: 0.2667\n",
      "Epoch [28/50], Step [101/531], Loss: 0.6333, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [28/50], Step [121/531], Loss: 0.6346, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.6667\n",
      "Epoch [28/50], Step [141/531], Loss: 0.7190, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.5000\n",
      "Epoch [28/50], Step [161/531], Loss: 0.7481, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.6000\n",
      "Epoch [28/50], Step [181/531], Loss: 0.7863, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 7.0000, batch_f1_score: 0.3333\n",
      "Epoch [28/50], Step [201/531], Loss: 0.6618, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [28/50], Step [221/531], Loss: 0.7164, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [28/50], Step [241/531], Loss: 0.7009, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [28/50], Step [261/531], Loss: 0.6693, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [28/50], Step [281/531], Loss: 0.6595, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [28/50], Step [301/531], Loss: 0.6728, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.4286\n",
      "Epoch [28/50], Step [321/531], Loss: 0.7481, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [28/50], Step [341/531], Loss: 0.7002, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [28/50], Step [361/531], Loss: 0.6179, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [28/50], Step [381/531], Loss: 0.6516, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7368\n",
      "Epoch [28/50], Step [401/531], Loss: 0.7465, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [28/50], Step [421/531], Loss: 0.6851, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [28/50], Step [441/531], Loss: 0.6378, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7619\n",
      "Epoch [28/50], Step [461/531], Loss: 0.6365, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [28/50], Step [481/531], Loss: 0.6077, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [28/50], Step [501/531], Loss: 0.6351, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [28/50], Step [521/531], Loss: 0.6338, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [29/50], Step [1/531], Loss: 0.6951, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [29/50], Step [21/531], Loss: 0.6502, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [29/50], Step [41/531], Loss: 0.6297, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [29/50], Step [61/531], Loss: 0.6628, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [29/50], Step [81/531], Loss: 0.6458, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [29/50], Step [101/531], Loss: 0.6918, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [29/50], Step [121/531], Loss: 0.6413, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [29/50], Step [141/531], Loss: 0.6288, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [29/50], Step [161/531], Loss: 0.6862, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [29/50], Step [181/531], Loss: 0.6409, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5714\n",
      "Epoch [29/50], Step [201/531], Loss: 0.7359, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [29/50], Step [221/531], Loss: 0.6967, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [29/50], Step [241/531], Loss: 0.7078, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.4615\n",
      "Epoch [29/50], Step [261/531], Loss: 0.5993, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7143\n",
      "Epoch [29/50], Step [281/531], Loss: 0.5392, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 0.0000, batch_f1_score: 0.9412\n",
      "Epoch [29/50], Step [301/531], Loss: 0.6040, tp_sum: 9.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8182\n",
      "Epoch [29/50], Step [321/531], Loss: 0.6402, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [29/50], Step [341/531], Loss: 0.7083, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [29/50], Step [361/531], Loss: 0.6006, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [29/50], Step [381/531], Loss: 0.7065, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [29/50], Step [401/531], Loss: 0.6396, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [29/50], Step [421/531], Loss: 0.6402, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [29/50], Step [441/531], Loss: 0.7080, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [29/50], Step [461/531], Loss: 0.6047, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [29/50], Step [481/531], Loss: 0.6768, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.6316\n",
      "Epoch [29/50], Step [501/531], Loss: 0.7197, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5263\n",
      "Epoch [29/50], Step [521/531], Loss: 0.6572, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [30/50], Step [1/531], Loss: 0.5721, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8889\n",
      "Epoch [30/50], Step [21/531], Loss: 0.7377, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [30/50], Step [41/531], Loss: 0.7043, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [30/50], Step [61/531], Loss: 0.7044, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [30/50], Step [81/531], Loss: 0.6975, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [30/50], Step [101/531], Loss: 0.6340, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [30/50], Step [121/531], Loss: 0.7040, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [30/50], Step [141/531], Loss: 0.7288, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5263\n",
      "Epoch [30/50], Step [161/531], Loss: 0.7272, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [30/50], Step [181/531], Loss: 0.6514, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8182\n",
      "Epoch [30/50], Step [201/531], Loss: 0.6995, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [30/50], Step [221/531], Loss: 0.7249, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [30/50], Step [241/531], Loss: 0.7177, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6957\n",
      "Epoch [30/50], Step [261/531], Loss: 0.7345, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [30/50], Step [281/531], Loss: 0.6030, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7500\n",
      "Epoch [30/50], Step [301/531], Loss: 0.7236, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [30/50], Step [321/531], Loss: 0.5935, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 0.0000, batch_f1_score: 0.7692\n",
      "Epoch [30/50], Step [341/531], Loss: 0.6389, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [30/50], Step [361/531], Loss: 0.6116, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [30/50], Step [381/531], Loss: 0.5865, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7778\n",
      "Epoch [30/50], Step [401/531], Loss: 0.6422, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6154\n",
      "Epoch [30/50], Step [421/531], Loss: 0.6240, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [30/50], Step [441/531], Loss: 0.7073, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [30/50], Step [461/531], Loss: 0.6280, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [30/50], Step [481/531], Loss: 0.7070, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [30/50], Step [501/531], Loss: 0.6666, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [30/50], Step [521/531], Loss: 0.6547, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [31/50], Step [1/531], Loss: 0.6398, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [31/50], Step [21/531], Loss: 0.6384, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [31/50], Step [41/531], Loss: 0.6523, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [31/50], Step [61/531], Loss: 0.6333, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [31/50], Step [81/531], Loss: 0.6316, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7143\n",
      "Epoch [31/50], Step [101/531], Loss: 0.6181, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [31/50], Step [121/531], Loss: 0.6676, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [31/50], Step [141/531], Loss: 0.7316, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [31/50], Step [161/531], Loss: 0.7184, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [31/50], Step [181/531], Loss: 0.6696, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [31/50], Step [201/531], Loss: 0.6567, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [31/50], Step [221/531], Loss: 0.7007, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [31/50], Step [241/531], Loss: 0.7414, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [31/50], Step [261/531], Loss: 0.6648, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7619\n",
      "Epoch [31/50], Step [281/531], Loss: 0.7016, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [31/50], Step [301/531], Loss: 0.6796, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.6250\n",
      "Epoch [31/50], Step [321/531], Loss: 0.6622, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [31/50], Step [341/531], Loss: 0.6136, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [31/50], Step [361/531], Loss: 0.6192, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [31/50], Step [381/531], Loss: 0.6706, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [31/50], Step [401/531], Loss: 0.6637, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.4286\n",
      "Epoch [31/50], Step [421/531], Loss: 0.6708, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [31/50], Step [441/531], Loss: 0.7305, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.6316\n",
      "Epoch [31/50], Step [461/531], Loss: 0.6199, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [31/50], Step [481/531], Loss: 0.7104, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [31/50], Step [501/531], Loss: 0.7268, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [31/50], Step [521/531], Loss: 0.6737, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [32/50], Step [1/531], Loss: 0.6845, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [32/50], Step [21/531], Loss: 0.6644, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.3636\n",
      "Epoch [32/50], Step [41/531], Loss: 0.6209, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [32/50], Step [61/531], Loss: 0.6296, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [32/50], Step [81/531], Loss: 0.6598, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [32/50], Step [101/531], Loss: 0.6301, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [32/50], Step [121/531], Loss: 0.7027, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4706\n",
      "Epoch [32/50], Step [141/531], Loss: 0.6417, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [32/50], Step [161/531], Loss: 0.6321, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [32/50], Step [181/531], Loss: 0.7019, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [32/50], Step [201/531], Loss: 0.7434, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.4286\n",
      "Epoch [32/50], Step [221/531], Loss: 0.7111, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [32/50], Step [241/531], Loss: 0.7145, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [32/50], Step [261/531], Loss: 0.6351, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [32/50], Step [281/531], Loss: 0.6751, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [32/50], Step [301/531], Loss: 0.6598, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [32/50], Step [321/531], Loss: 0.6044, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8235\n",
      "Epoch [32/50], Step [341/531], Loss: 0.6100, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [32/50], Step [361/531], Loss: 0.6877, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [32/50], Step [381/531], Loss: 0.7114, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.3750\n",
      "Epoch [32/50], Step [401/531], Loss: 0.7586, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.3750\n",
      "Epoch [32/50], Step [421/531], Loss: 0.7105, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [32/50], Step [441/531], Loss: 0.7951, tp_sum: 4.0000, fp_sum: 7.0000, fn_sum: 4.0000, batch_f1_score: 0.4211\n",
      "Epoch [32/50], Step [461/531], Loss: 0.6774, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [32/50], Step [481/531], Loss: 0.6382, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [32/50], Step [501/531], Loss: 0.7084, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [32/50], Step [521/531], Loss: 0.6291, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [33/50], Step [1/531], Loss: 0.7199, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [33/50], Step [21/531], Loss: 0.6782, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [33/50], Step [41/531], Loss: 0.6721, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [33/50], Step [61/531], Loss: 0.6400, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [33/50], Step [81/531], Loss: 0.6937, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [33/50], Step [101/531], Loss: 0.6175, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [33/50], Step [121/531], Loss: 0.7480, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n",
      "Epoch [33/50], Step [141/531], Loss: 0.6551, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [33/50], Step [161/531], Loss: 0.6746, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [33/50], Step [181/531], Loss: 0.6890, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [33/50], Step [201/531], Loss: 0.6913, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [33/50], Step [221/531], Loss: 0.7441, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [33/50], Step [241/531], Loss: 0.5898, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [33/50], Step [261/531], Loss: 0.6520, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [33/50], Step [281/531], Loss: 0.6152, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [33/50], Step [301/531], Loss: 0.7009, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [33/50], Step [321/531], Loss: 0.7648, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [33/50], Step [341/531], Loss: 0.6365, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [33/50], Step [361/531], Loss: 0.8079, tp_sum: 1.0000, fp_sum: 7.0000, fn_sum: 5.0000, batch_f1_score: 0.1429\n",
      "Epoch [33/50], Step [381/531], Loss: 0.6510, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [33/50], Step [401/531], Loss: 0.6960, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5263\n",
      "Epoch [33/50], Step [421/531], Loss: 0.7353, tp_sum: 1.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.2222\n",
      "Epoch [33/50], Step [441/531], Loss: 0.7177, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [33/50], Step [461/531], Loss: 0.6162, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [33/50], Step [481/531], Loss: 0.6564, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [33/50], Step [501/531], Loss: 0.6697, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [33/50], Step [521/531], Loss: 0.7378, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [34/50], Step [1/531], Loss: 0.6133, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [34/50], Step [21/531], Loss: 0.5810, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [34/50], Step [41/531], Loss: 0.6800, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [34/50], Step [61/531], Loss: 0.5694, tp_sum: 7.0000, fp_sum: 0.0000, fn_sum: 4.0000, batch_f1_score: 0.7778\n",
      "Epoch [34/50], Step [81/531], Loss: 0.6439, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [34/50], Step [101/531], Loss: 0.5149, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8421\n",
      "Epoch [34/50], Step [121/531], Loss: 0.6368, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [34/50], Step [141/531], Loss: 0.6567, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [34/50], Step [161/531], Loss: 0.7032, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [34/50], Step [181/531], Loss: 0.7329, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [34/50], Step [201/531], Loss: 0.5947, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7143\n",
      "Epoch [34/50], Step [221/531], Loss: 0.6124, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8750\n",
      "Epoch [34/50], Step [241/531], Loss: 0.6927, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [34/50], Step [261/531], Loss: 0.6373, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [34/50], Step [281/531], Loss: 0.7718, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n",
      "Epoch [34/50], Step [301/531], Loss: 0.7802, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.4444\n",
      "Epoch [34/50], Step [321/531], Loss: 0.6251, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [34/50], Step [341/531], Loss: 0.6909, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5714\n",
      "Epoch [34/50], Step [361/531], Loss: 0.6351, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8421\n",
      "Epoch [34/50], Step [381/531], Loss: 0.6613, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [34/50], Step [401/531], Loss: 0.6337, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [34/50], Step [421/531], Loss: 0.7333, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [34/50], Step [441/531], Loss: 0.5870, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8750\n",
      "Epoch [34/50], Step [461/531], Loss: 0.6189, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [34/50], Step [481/531], Loss: 0.7156, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.5000\n",
      "Epoch [34/50], Step [501/531], Loss: 0.7002, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [34/50], Step [521/531], Loss: 0.6957, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [35/50], Step [1/531], Loss: 0.7713, tp_sum: 2.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.3077\n",
      "Epoch [35/50], Step [21/531], Loss: 0.7849, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 5.0000, batch_f1_score: 0.3333\n",
      "Epoch [35/50], Step [41/531], Loss: 0.6199, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [35/50], Step [61/531], Loss: 0.7182, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [35/50], Step [81/531], Loss: 0.7374, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.3333\n",
      "Epoch [35/50], Step [101/531], Loss: 0.6445, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [35/50], Step [121/531], Loss: 0.6773, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [35/50], Step [141/531], Loss: 0.6669, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [35/50], Step [161/531], Loss: 0.6799, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [35/50], Step [181/531], Loss: 0.6358, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [35/50], Step [201/531], Loss: 0.6234, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [35/50], Step [221/531], Loss: 0.6480, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [35/50], Step [241/531], Loss: 0.6748, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [35/50], Step [261/531], Loss: 0.7320, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [35/50], Step [281/531], Loss: 0.7255, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [35/50], Step [301/531], Loss: 0.5886, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [35/50], Step [321/531], Loss: 0.5865, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7143\n",
      "Epoch [35/50], Step [341/531], Loss: 0.6565, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [35/50], Step [361/531], Loss: 0.6474, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [35/50], Step [381/531], Loss: 0.6139, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [35/50], Step [401/531], Loss: 0.6346, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.7000\n",
      "Epoch [35/50], Step [421/531], Loss: 0.6539, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [35/50], Step [441/531], Loss: 0.5832, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5714\n",
      "Epoch [35/50], Step [461/531], Loss: 0.7065, tp_sum: 2.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.3333\n",
      "Epoch [35/50], Step [481/531], Loss: 0.6923, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [35/50], Step [501/531], Loss: 0.7436, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.4706\n",
      "Epoch [35/50], Step [521/531], Loss: 0.7991, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.2857\n",
      "Epoch [36/50], Step [1/531], Loss: 0.6548, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [36/50], Step [21/531], Loss: 0.7426, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [36/50], Step [41/531], Loss: 0.6041, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [36/50], Step [61/531], Loss: 0.6278, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6667\n",
      "Epoch [36/50], Step [81/531], Loss: 0.7815, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.2857\n",
      "Epoch [36/50], Step [101/531], Loss: 0.7688, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [36/50], Step [121/531], Loss: 0.6670, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [36/50], Step [141/531], Loss: 0.7470, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [36/50], Step [161/531], Loss: 0.5839, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8889\n",
      "Epoch [36/50], Step [181/531], Loss: 0.6447, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [36/50], Step [201/531], Loss: 0.7121, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [36/50], Step [221/531], Loss: 0.6166, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [36/50], Step [241/531], Loss: 0.5857, tp_sum: 8.0000, fp_sum: 0.0000, fn_sum: 2.0000, batch_f1_score: 0.8889\n",
      "Epoch [36/50], Step [261/531], Loss: 0.6654, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [36/50], Step [281/531], Loss: 0.6488, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [36/50], Step [301/531], Loss: 0.7045, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [36/50], Step [321/531], Loss: 0.6285, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7778\n",
      "Epoch [36/50], Step [341/531], Loss: 0.6144, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [36/50], Step [361/531], Loss: 0.6567, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [36/50], Step [381/531], Loss: 0.6265, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [36/50], Step [401/531], Loss: 0.6258, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7500\n",
      "Epoch [36/50], Step [421/531], Loss: 0.7335, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [36/50], Step [441/531], Loss: 0.7067, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5714\n",
      "Epoch [36/50], Step [461/531], Loss: 0.7454, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [36/50], Step [481/531], Loss: 0.5321, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8421\n",
      "Epoch [36/50], Step [501/531], Loss: 0.5532, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8421\n",
      "Epoch [36/50], Step [521/531], Loss: 0.6878, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [37/50], Step [1/531], Loss: 0.7382, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.2857\n",
      "Epoch [37/50], Step [21/531], Loss: 0.6639, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [37/50], Step [41/531], Loss: 0.6205, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [37/50], Step [61/531], Loss: 0.5991, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [37/50], Step [81/531], Loss: 0.6390, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [37/50], Step [101/531], Loss: 0.5867, tp_sum: 5.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7143\n",
      "Epoch [37/50], Step [121/531], Loss: 0.6742, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.4615\n",
      "Epoch [37/50], Step [141/531], Loss: 0.7142, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [37/50], Step [161/531], Loss: 0.7629, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [37/50], Step [181/531], Loss: 0.6500, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [37/50], Step [201/531], Loss: 0.7546, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4000\n",
      "Epoch [37/50], Step [221/531], Loss: 0.6102, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8571\n",
      "Epoch [37/50], Step [241/531], Loss: 0.6232, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7273\n",
      "Epoch [37/50], Step [261/531], Loss: 0.6401, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8421\n",
      "Epoch [37/50], Step [281/531], Loss: 0.6887, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.3333\n",
      "Epoch [37/50], Step [301/531], Loss: 0.6949, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6957\n",
      "Epoch [37/50], Step [321/531], Loss: 0.8292, tp_sum: 1.0000, fp_sum: 8.0000, fn_sum: 3.0000, batch_f1_score: 0.1538\n",
      "Epoch [37/50], Step [341/531], Loss: 0.6374, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [37/50], Step [361/531], Loss: 0.7117, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [37/50], Step [381/531], Loss: 0.6537, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.6667\n",
      "Epoch [37/50], Step [401/531], Loss: 0.6552, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [37/50], Step [421/531], Loss: 0.6523, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [37/50], Step [441/531], Loss: 0.6853, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.6000\n",
      "Epoch [37/50], Step [461/531], Loss: 0.6634, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [37/50], Step [481/531], Loss: 0.6856, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7059\n",
      "Epoch [37/50], Step [501/531], Loss: 0.6409, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [37/50], Step [521/531], Loss: 0.6202, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [38/50], Step [1/531], Loss: 0.7196, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [38/50], Step [21/531], Loss: 0.6502, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [38/50], Step [41/531], Loss: 0.6919, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [38/50], Step [61/531], Loss: 0.7575, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.5263\n",
      "Epoch [38/50], Step [81/531], Loss: 0.6125, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [38/50], Step [101/531], Loss: 0.6140, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [38/50], Step [121/531], Loss: 0.6498, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.7000\n",
      "Epoch [38/50], Step [141/531], Loss: 0.6504, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [38/50], Step [161/531], Loss: 0.6743, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6667\n",
      "Epoch [38/50], Step [181/531], Loss: 0.6632, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [38/50], Step [201/531], Loss: 0.6867, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.5000\n",
      "Epoch [38/50], Step [221/531], Loss: 0.6447, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [38/50], Step [241/531], Loss: 0.6434, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [38/50], Step [261/531], Loss: 0.7343, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [38/50], Step [281/531], Loss: 0.6351, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.6667\n",
      "Epoch [38/50], Step [301/531], Loss: 0.6136, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [38/50], Step [321/531], Loss: 0.6554, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.6316\n",
      "Epoch [38/50], Step [341/531], Loss: 0.7044, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5263\n",
      "Epoch [38/50], Step [361/531], Loss: 0.6161, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6667\n",
      "Epoch [38/50], Step [381/531], Loss: 0.6845, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [38/50], Step [401/531], Loss: 0.6391, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [38/50], Step [421/531], Loss: 0.7638, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [38/50], Step [441/531], Loss: 0.7651, tp_sum: 1.0000, fp_sum: 8.0000, fn_sum: 4.0000, batch_f1_score: 0.1429\n",
      "Epoch [38/50], Step [461/531], Loss: 0.6298, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 0.0000, batch_f1_score: 0.5000\n",
      "Epoch [38/50], Step [481/531], Loss: 0.6694, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [38/50], Step [501/531], Loss: 0.6174, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [38/50], Step [521/531], Loss: 0.6554, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.4000\n",
      "Epoch [39/50], Step [1/531], Loss: 0.6536, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [39/50], Step [21/531], Loss: 0.7052, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.5263\n",
      "Epoch [39/50], Step [41/531], Loss: 0.6823, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [39/50], Step [61/531], Loss: 0.7249, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 7.0000, batch_f1_score: 0.5000\n",
      "Epoch [39/50], Step [81/531], Loss: 0.6565, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [39/50], Step [101/531], Loss: 0.6776, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [39/50], Step [121/531], Loss: 0.7261, tp_sum: 0.0000, fp_sum: 8.0000, fn_sum: 2.0000, batch_f1_score: 0.0000\n",
      "Epoch [39/50], Step [141/531], Loss: 0.6855, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5333\n",
      "Epoch [39/50], Step [161/531], Loss: 0.6337, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [39/50], Step [181/531], Loss: 0.6878, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.4286\n",
      "Epoch [39/50], Step [201/531], Loss: 0.6340, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [39/50], Step [221/531], Loss: 0.6970, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [39/50], Step [241/531], Loss: 0.7370, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [39/50], Step [261/531], Loss: 0.7497, tp_sum: 1.0000, fp_sum: 7.0000, fn_sum: 2.0000, batch_f1_score: 0.1818\n",
      "Epoch [39/50], Step [281/531], Loss: 0.7070, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [39/50], Step [301/531], Loss: 0.7012, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [39/50], Step [321/531], Loss: 0.5773, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8889\n",
      "Epoch [39/50], Step [341/531], Loss: 0.7437, tp_sum: 1.0000, fp_sum: 8.0000, fn_sum: 2.0000, batch_f1_score: 0.1667\n",
      "Epoch [39/50], Step [361/531], Loss: 0.6554, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [39/50], Step [381/531], Loss: 0.6871, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [39/50], Step [401/531], Loss: 0.7368, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [39/50], Step [421/531], Loss: 0.6242, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [39/50], Step [441/531], Loss: 0.6417, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [39/50], Step [461/531], Loss: 0.6668, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.5263\n",
      "Epoch [39/50], Step [481/531], Loss: 0.7128, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [39/50], Step [501/531], Loss: 0.6197, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7059\n",
      "Epoch [39/50], Step [521/531], Loss: 0.6427, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [40/50], Step [1/531], Loss: 0.6786, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [40/50], Step [21/531], Loss: 0.7228, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6364\n",
      "Epoch [40/50], Step [41/531], Loss: 0.7243, tp_sum: 1.0000, fp_sum: 7.0000, fn_sum: 3.0000, batch_f1_score: 0.1667\n",
      "Epoch [40/50], Step [61/531], Loss: 0.6774, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [40/50], Step [81/531], Loss: 0.7545, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [40/50], Step [101/531], Loss: 0.6028, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [40/50], Step [121/531], Loss: 0.6844, tp_sum: 9.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8182\n",
      "Epoch [40/50], Step [141/531], Loss: 0.6781, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [40/50], Step [161/531], Loss: 0.6686, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [40/50], Step [181/531], Loss: 0.6444, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [40/50], Step [201/531], Loss: 0.6056, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 0.0000, batch_f1_score: 0.8750\n",
      "Epoch [40/50], Step [221/531], Loss: 0.7125, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [40/50], Step [241/531], Loss: 0.6558, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [40/50], Step [261/531], Loss: 0.6853, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [40/50], Step [281/531], Loss: 0.6194, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [40/50], Step [301/531], Loss: 0.6504, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [40/50], Step [321/531], Loss: 0.6645, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [40/50], Step [341/531], Loss: 0.7076, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.4615\n",
      "Epoch [40/50], Step [361/531], Loss: 0.6171, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [40/50], Step [381/531], Loss: 0.6493, tp_sum: 2.0000, fp_sum: 8.0000, fn_sum: 1.0000, batch_f1_score: 0.3077\n",
      "Epoch [40/50], Step [401/531], Loss: 0.6576, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6316\n",
      "Epoch [40/50], Step [421/531], Loss: 0.6735, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [40/50], Step [441/531], Loss: 0.6792, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [40/50], Step [461/531], Loss: 0.6996, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [40/50], Step [481/531], Loss: 0.6976, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [40/50], Step [501/531], Loss: 0.7198, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.5000\n",
      "Epoch [40/50], Step [521/531], Loss: 0.6399, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [41/50], Step [1/531], Loss: 0.6428, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [41/50], Step [21/531], Loss: 0.7164, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.6000\n",
      "Epoch [41/50], Step [41/531], Loss: 0.6556, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.4615\n",
      "Epoch [41/50], Step [61/531], Loss: 0.6076, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [41/50], Step [81/531], Loss: 0.6352, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7619\n",
      "Epoch [41/50], Step [101/531], Loss: 0.7237, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [41/50], Step [121/531], Loss: 0.6281, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [41/50], Step [141/531], Loss: 0.5937, tp_sum: 9.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.8182\n",
      "Epoch [41/50], Step [161/531], Loss: 0.7357, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.3529\n",
      "Epoch [41/50], Step [181/531], Loss: 0.7334, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [41/50], Step [201/531], Loss: 0.6385, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [41/50], Step [221/531], Loss: 0.6006, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [41/50], Step [241/531], Loss: 0.6339, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [41/50], Step [261/531], Loss: 0.7467, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.6000\n",
      "Epoch [41/50], Step [281/531], Loss: 0.6043, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [41/50], Step [301/531], Loss: 0.6966, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [41/50], Step [321/531], Loss: 0.6639, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6667\n",
      "Epoch [41/50], Step [341/531], Loss: 0.6927, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [41/50], Step [361/531], Loss: 0.7273, tp_sum: 2.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.3077\n",
      "Epoch [41/50], Step [381/531], Loss: 0.6232, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [41/50], Step [401/531], Loss: 0.7131, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [41/50], Step [421/531], Loss: 0.6736, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [41/50], Step [441/531], Loss: 0.6844, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [41/50], Step [461/531], Loss: 0.6074, tp_sum: 5.0000, fp_sum: 0.0000, fn_sum: 5.0000, batch_f1_score: 0.6667\n",
      "Epoch [41/50], Step [481/531], Loss: 0.6239, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [41/50], Step [501/531], Loss: 0.5883, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [41/50], Step [521/531], Loss: 0.7103, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [42/50], Step [1/531], Loss: 0.6145, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [42/50], Step [21/531], Loss: 0.5982, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [42/50], Step [41/531], Loss: 0.7966, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.3077\n",
      "Epoch [42/50], Step [61/531], Loss: 0.6449, tp_sum: 3.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.4615\n",
      "Epoch [42/50], Step [81/531], Loss: 0.6376, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [42/50], Step [101/531], Loss: 0.6484, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7619\n",
      "Epoch [42/50], Step [121/531], Loss: 0.6177, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [42/50], Step [141/531], Loss: 0.7250, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [42/50], Step [161/531], Loss: 0.7748, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.2857\n",
      "Epoch [42/50], Step [181/531], Loss: 0.6418, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.5000\n",
      "Epoch [42/50], Step [201/531], Loss: 0.6696, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.3333\n",
      "Epoch [42/50], Step [221/531], Loss: 0.7183, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [42/50], Step [241/531], Loss: 0.6808, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [42/50], Step [261/531], Loss: 0.6161, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [42/50], Step [281/531], Loss: 0.5976, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [42/50], Step [301/531], Loss: 0.6750, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [42/50], Step [321/531], Loss: 0.7245, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [42/50], Step [341/531], Loss: 0.6629, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [42/50], Step [361/531], Loss: 0.6703, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [42/50], Step [381/531], Loss: 0.6664, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [42/50], Step [401/531], Loss: 0.6542, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [42/50], Step [421/531], Loss: 0.7300, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [42/50], Step [441/531], Loss: 0.6466, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [42/50], Step [461/531], Loss: 0.6170, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6154\n",
      "Epoch [42/50], Step [481/531], Loss: 0.6156, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6667\n",
      "Epoch [42/50], Step [501/531], Loss: 0.7029, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [42/50], Step [521/531], Loss: 0.6689, tp_sum: 6.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.6000\n",
      "Epoch [43/50], Step [1/531], Loss: 0.7446, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [43/50], Step [21/531], Loss: 0.6513, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [43/50], Step [41/531], Loss: 0.6411, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [43/50], Step [61/531], Loss: 0.6587, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [43/50], Step [81/531], Loss: 0.6655, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [43/50], Step [101/531], Loss: 0.6640, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [43/50], Step [121/531], Loss: 0.6398, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [43/50], Step [141/531], Loss: 0.5990, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8000\n",
      "Epoch [43/50], Step [161/531], Loss: 0.6327, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [43/50], Step [181/531], Loss: 0.5876, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7143\n",
      "Epoch [43/50], Step [201/531], Loss: 0.7230, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [43/50], Step [221/531], Loss: 0.6614, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 0.0000, batch_f1_score: 0.5000\n",
      "Epoch [43/50], Step [241/531], Loss: 0.6959, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [43/50], Step [261/531], Loss: 0.6289, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [43/50], Step [281/531], Loss: 0.6972, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6667\n",
      "Epoch [43/50], Step [301/531], Loss: 0.6206, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8182\n",
      "Epoch [43/50], Step [321/531], Loss: 0.6048, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7500\n",
      "Epoch [43/50], Step [341/531], Loss: 0.7438, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5263\n",
      "Epoch [43/50], Step [361/531], Loss: 0.7009, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.4286\n",
      "Epoch [43/50], Step [381/531], Loss: 0.6140, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [43/50], Step [401/531], Loss: 0.6646, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [43/50], Step [421/531], Loss: 0.7253, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [43/50], Step [441/531], Loss: 0.6065, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 0.0000, batch_f1_score: 0.8333\n",
      "Epoch [43/50], Step [461/531], Loss: 0.7764, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [43/50], Step [481/531], Loss: 0.6707, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5882\n",
      "Epoch [43/50], Step [501/531], Loss: 0.6234, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7778\n",
      "Epoch [43/50], Step [521/531], Loss: 0.7071, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [44/50], Step [1/531], Loss: 0.6925, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [44/50], Step [21/531], Loss: 0.6020, tp_sum: 8.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7619\n",
      "Epoch [44/50], Step [41/531], Loss: 0.5734, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [44/50], Step [61/531], Loss: 0.7303, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [44/50], Step [81/531], Loss: 0.6414, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [44/50], Step [101/531], Loss: 0.7159, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.6667\n",
      "Epoch [44/50], Step [121/531], Loss: 0.6673, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 0.0000, batch_f1_score: 0.4615\n",
      "Epoch [44/50], Step [141/531], Loss: 0.6349, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [44/50], Step [161/531], Loss: 0.6765, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [44/50], Step [181/531], Loss: 0.6619, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [44/50], Step [201/531], Loss: 0.7328, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [44/50], Step [221/531], Loss: 0.6580, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [44/50], Step [241/531], Loss: 0.7663, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.3529\n",
      "Epoch [44/50], Step [261/531], Loss: 0.6861, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.7000\n",
      "Epoch [44/50], Step [281/531], Loss: 0.7173, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [44/50], Step [301/531], Loss: 0.6619, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [44/50], Step [321/531], Loss: 0.7225, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.3529\n",
      "Epoch [44/50], Step [341/531], Loss: 0.5812, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [44/50], Step [361/531], Loss: 0.7322, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 5.0000, batch_f1_score: 0.3529\n",
      "Epoch [44/50], Step [381/531], Loss: 0.5689, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [44/50], Step [401/531], Loss: 0.6200, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7368\n",
      "Epoch [44/50], Step [421/531], Loss: 0.6207, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7619\n",
      "Epoch [44/50], Step [441/531], Loss: 0.5629, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 1.0000, batch_f1_score: 0.8571\n",
      "Epoch [44/50], Step [461/531], Loss: 0.7420, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.5263\n",
      "Epoch [44/50], Step [481/531], Loss: 0.6575, tp_sum: 3.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.4615\n",
      "Epoch [44/50], Step [501/531], Loss: 0.6181, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.7059\n",
      "Epoch [44/50], Step [521/531], Loss: 0.6953, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [45/50], Step [1/531], Loss: 0.6280, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [45/50], Step [21/531], Loss: 0.6505, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.5455\n",
      "Epoch [45/50], Step [41/531], Loss: 0.7235, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [45/50], Step [61/531], Loss: 0.7613, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.3750\n",
      "Epoch [45/50], Step [81/531], Loss: 0.5671, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7778\n",
      "Epoch [45/50], Step [101/531], Loss: 0.6600, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [45/50], Step [121/531], Loss: 0.7179, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [45/50], Step [141/531], Loss: 0.6739, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.4286\n",
      "Epoch [45/50], Step [161/531], Loss: 0.6613, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [45/50], Step [181/531], Loss: 0.6564, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [45/50], Step [201/531], Loss: 0.6922, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [45/50], Step [221/531], Loss: 0.6858, tp_sum: 9.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7826\n",
      "Epoch [45/50], Step [241/531], Loss: 0.6555, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [45/50], Step [261/531], Loss: 0.7508, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.4444\n",
      "Epoch [45/50], Step [281/531], Loss: 0.5990, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [45/50], Step [301/531], Loss: 0.7082, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [45/50], Step [321/531], Loss: 0.6493, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [45/50], Step [341/531], Loss: 0.6079, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.6154\n",
      "Epoch [45/50], Step [361/531], Loss: 0.6674, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [45/50], Step [381/531], Loss: 0.6581, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [45/50], Step [401/531], Loss: 0.6352, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [45/50], Step [421/531], Loss: 0.6242, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [45/50], Step [441/531], Loss: 0.7036, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [45/50], Step [461/531], Loss: 0.7095, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [45/50], Step [481/531], Loss: 0.6096, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7619\n",
      "Epoch [45/50], Step [501/531], Loss: 0.6756, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.7500\n",
      "Epoch [45/50], Step [521/531], Loss: 0.7248, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5000\n",
      "Epoch [46/50], Step [1/531], Loss: 0.6993, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [46/50], Step [21/531], Loss: 0.7017, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 6.0000, batch_f1_score: 0.6000\n",
      "Epoch [46/50], Step [41/531], Loss: 0.5268, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8421\n",
      "Epoch [46/50], Step [61/531], Loss: 0.5906, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8000\n",
      "Epoch [46/50], Step [81/531], Loss: 0.6735, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [46/50], Step [101/531], Loss: 0.7311, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [46/50], Step [121/531], Loss: 0.7972, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 7.0000, batch_f1_score: 0.4762\n",
      "Epoch [46/50], Step [141/531], Loss: 0.6557, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [46/50], Step [161/531], Loss: 0.7147, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [46/50], Step [181/531], Loss: 0.7174, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [46/50], Step [201/531], Loss: 0.6175, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7143\n",
      "Epoch [46/50], Step [221/531], Loss: 0.6638, tp_sum: 9.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7826\n",
      "Epoch [46/50], Step [241/531], Loss: 0.5945, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [46/50], Step [261/531], Loss: 0.6531, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [46/50], Step [281/531], Loss: 0.6425, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [46/50], Step [301/531], Loss: 0.6262, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n",
      "Epoch [46/50], Step [321/531], Loss: 0.6798, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5000\n",
      "Epoch [46/50], Step [341/531], Loss: 0.6785, tp_sum: 2.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.3333\n",
      "Epoch [46/50], Step [361/531], Loss: 0.6496, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [46/50], Step [381/531], Loss: 0.6240, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.4615\n",
      "Epoch [46/50], Step [401/531], Loss: 0.7084, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [46/50], Step [421/531], Loss: 0.6463, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 0.0000, batch_f1_score: 0.5714\n",
      "Epoch [46/50], Step [441/531], Loss: 0.6368, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [46/50], Step [461/531], Loss: 0.6340, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [46/50], Step [481/531], Loss: 0.7475, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4706\n",
      "Epoch [46/50], Step [501/531], Loss: 0.6926, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.3636\n",
      "Epoch [46/50], Step [521/531], Loss: 0.6384, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [47/50], Step [1/531], Loss: 0.6556, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [47/50], Step [21/531], Loss: 0.6969, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [47/50], Step [41/531], Loss: 0.7164, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [47/50], Step [61/531], Loss: 0.6847, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4706\n",
      "Epoch [47/50], Step [81/531], Loss: 0.6757, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [47/50], Step [101/531], Loss: 0.6457, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [47/50], Step [121/531], Loss: 0.6512, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5333\n",
      "Epoch [47/50], Step [141/531], Loss: 0.6904, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 6.0000, batch_f1_score: 0.3529\n",
      "Epoch [47/50], Step [161/531], Loss: 0.6334, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [47/50], Step [181/531], Loss: 0.6399, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8000\n",
      "Epoch [47/50], Step [201/531], Loss: 0.6427, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7368\n",
      "Epoch [47/50], Step [221/531], Loss: 0.6696, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.5714\n",
      "Epoch [47/50], Step [241/531], Loss: 0.6822, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [47/50], Step [261/531], Loss: 0.6662, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [47/50], Step [281/531], Loss: 0.6424, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7059\n",
      "Epoch [47/50], Step [301/531], Loss: 0.6310, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6154\n",
      "Epoch [47/50], Step [321/531], Loss: 0.6379, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 1.0000, batch_f1_score: 0.5333\n",
      "Epoch [47/50], Step [341/531], Loss: 0.6414, tp_sum: 6.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.7059\n",
      "Epoch [47/50], Step [361/531], Loss: 0.6440, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [47/50], Step [381/531], Loss: 0.6459, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7368\n",
      "Epoch [47/50], Step [401/531], Loss: 0.6961, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [47/50], Step [421/531], Loss: 0.7114, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 5.0000, batch_f1_score: 0.5882\n",
      "Epoch [47/50], Step [441/531], Loss: 0.7806, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [47/50], Step [461/531], Loss: 0.6836, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6667\n",
      "Epoch [47/50], Step [481/531], Loss: 0.6769, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.5714\n",
      "Epoch [47/50], Step [501/531], Loss: 0.6864, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [47/50], Step [521/531], Loss: 0.6656, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6250\n",
      "Epoch [48/50], Step [1/531], Loss: 0.7135, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 4.0000, batch_f1_score: 0.7619\n",
      "Epoch [48/50], Step [21/531], Loss: 0.6647, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [48/50], Step [41/531], Loss: 0.6582, tp_sum: 4.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.5714\n",
      "Epoch [48/50], Step [61/531], Loss: 0.6947, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [48/50], Step [81/531], Loss: 0.6529, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [48/50], Step [101/531], Loss: 0.7465, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [48/50], Step [121/531], Loss: 0.6472, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 5.0000, batch_f1_score: 0.7273\n",
      "Epoch [48/50], Step [141/531], Loss: 0.6541, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [48/50], Step [161/531], Loss: 0.7235, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [48/50], Step [181/531], Loss: 0.6320, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [48/50], Step [201/531], Loss: 0.7524, tp_sum: 5.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.5263\n",
      "Epoch [48/50], Step [221/531], Loss: 0.6742, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [48/50], Step [241/531], Loss: 0.6802, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [48/50], Step [261/531], Loss: 0.5725, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [48/50], Step [281/531], Loss: 0.7222, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 5.0000, batch_f1_score: 0.3529\n",
      "Epoch [48/50], Step [301/531], Loss: 0.5905, tp_sum: 8.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8421\n",
      "Epoch [48/50], Step [321/531], Loss: 0.6389, tp_sum: 9.0000, fp_sum: 0.0000, fn_sum: 4.0000, batch_f1_score: 0.8182\n",
      "Epoch [48/50], Step [341/531], Loss: 0.6672, tp_sum: 10.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8333\n",
      "Epoch [48/50], Step [361/531], Loss: 0.7348, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 6.0000, batch_f1_score: 0.5714\n",
      "Epoch [48/50], Step [381/531], Loss: 0.7371, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 5.0000, batch_f1_score: 0.5556\n",
      "Epoch [48/50], Step [401/531], Loss: 0.6211, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.6667\n",
      "Epoch [48/50], Step [421/531], Loss: 0.6403, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 0.0000, batch_f1_score: 0.5455\n",
      "Epoch [48/50], Step [441/531], Loss: 0.6679, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.6316\n",
      "Epoch [48/50], Step [461/531], Loss: 0.5968, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8235\n",
      "Epoch [48/50], Step [481/531], Loss: 0.7027, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 6.0000, batch_f1_score: 0.6364\n",
      "Epoch [48/50], Step [501/531], Loss: 0.6651, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 1.0000, batch_f1_score: 0.6250\n",
      "Epoch [48/50], Step [521/531], Loss: 0.7074, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5000\n",
      "Epoch [49/50], Step [1/531], Loss: 0.7080, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6667\n",
      "Epoch [49/50], Step [21/531], Loss: 0.6933, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 1.0000, batch_f1_score: 0.4286\n",
      "Epoch [49/50], Step [41/531], Loss: 0.6386, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 0.0000, batch_f1_score: 0.7500\n",
      "Epoch [49/50], Step [61/531], Loss: 0.6787, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.5882\n",
      "Epoch [49/50], Step [81/531], Loss: 0.7218, tp_sum: 3.0000, fp_sum: 4.0000, fn_sum: 5.0000, batch_f1_score: 0.4000\n",
      "Epoch [49/50], Step [101/531], Loss: 0.6676, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [49/50], Step [121/531], Loss: 0.6429, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7500\n",
      "Epoch [49/50], Step [141/531], Loss: 0.7207, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5556\n",
      "Epoch [49/50], Step [161/531], Loss: 0.6967, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.7000\n",
      "Epoch [49/50], Step [181/531], Loss: 0.7563, tp_sum: 2.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.3333\n",
      "Epoch [49/50], Step [201/531], Loss: 0.5976, tp_sum: 7.0000, fp_sum: 3.0000, fn_sum: 0.0000, batch_f1_score: 0.8235\n",
      "Epoch [49/50], Step [221/531], Loss: 0.7683, tp_sum: 1.0000, fp_sum: 8.0000, fn_sum: 3.0000, batch_f1_score: 0.1538\n",
      "Epoch [49/50], Step [241/531], Loss: 0.6704, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5333\n",
      "Epoch [49/50], Step [261/531], Loss: 0.6394, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 2.0000, batch_f1_score: 0.8235\n",
      "Epoch [49/50], Step [281/531], Loss: 0.6545, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 4.0000, batch_f1_score: 0.6316\n",
      "Epoch [49/50], Step [301/531], Loss: 0.6989, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 6.0000, batch_f1_score: 0.4444\n",
      "Epoch [49/50], Step [321/531], Loss: 0.6757, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [49/50], Step [341/531], Loss: 0.6989, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [49/50], Step [361/531], Loss: 0.7164, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [49/50], Step [381/531], Loss: 0.7298, tp_sum: 5.0000, fp_sum: 2.0000, fn_sum: 8.0000, batch_f1_score: 0.5000\n",
      "Epoch [49/50], Step [401/531], Loss: 0.6683, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.6250\n",
      "Epoch [49/50], Step [421/531], Loss: 0.6151, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.6154\n",
      "Epoch [49/50], Step [441/531], Loss: 0.7000, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.5556\n",
      "Epoch [49/50], Step [461/531], Loss: 0.6632, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [49/50], Step [481/531], Loss: 0.6747, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.5714\n",
      "Epoch [49/50], Step [501/531], Loss: 0.6830, tp_sum: 7.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.7000\n",
      "Epoch [49/50], Step [521/531], Loss: 0.6888, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [50/50], Step [1/531], Loss: 0.7108, tp_sum: 5.0000, fp_sum: 5.0000, fn_sum: 2.0000, batch_f1_score: 0.5882\n",
      "Epoch [50/50], Step [21/531], Loss: 0.6553, tp_sum: 4.0000, fp_sum: 3.0000, fn_sum: 3.0000, batch_f1_score: 0.5714\n",
      "Epoch [50/50], Step [41/531], Loss: 0.7005, tp_sum: 2.0000, fp_sum: 6.0000, fn_sum: 4.0000, batch_f1_score: 0.2857\n",
      "Epoch [50/50], Step [61/531], Loss: 0.7797, tp_sum: 3.0000, fp_sum: 7.0000, fn_sum: 5.0000, batch_f1_score: 0.3333\n",
      "Epoch [50/50], Step [81/531], Loss: 0.7396, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 3.0000, batch_f1_score: 0.4706\n",
      "Epoch [50/50], Step [101/531], Loss: 0.6984, tp_sum: 10.0000, fp_sum: 2.0000, fn_sum: 2.0000, batch_f1_score: 0.8333\n",
      "Epoch [50/50], Step [121/531], Loss: 0.6754, tp_sum: 6.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.7059\n",
      "Epoch [50/50], Step [141/531], Loss: 0.5928, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 1.0000, batch_f1_score: 0.8421\n",
      "Epoch [50/50], Step [161/531], Loss: 0.7008, tp_sum: 4.0000, fp_sum: 5.0000, fn_sum: 3.0000, batch_f1_score: 0.5000\n",
      "Epoch [50/50], Step [181/531], Loss: 0.6715, tp_sum: 8.0000, fp_sum: 2.0000, fn_sum: 3.0000, batch_f1_score: 0.7619\n",
      "Epoch [50/50], Step [201/531], Loss: 0.6773, tp_sum: 6.0000, fp_sum: 2.0000, fn_sum: 6.0000, batch_f1_score: 0.6000\n",
      "Epoch [50/50], Step [221/531], Loss: 0.5838, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 1.0000, batch_f1_score: 0.7143\n",
      "Epoch [50/50], Step [241/531], Loss: 0.6696, tp_sum: 6.0000, fp_sum: 1.0000, fn_sum: 6.0000, batch_f1_score: 0.6316\n",
      "Epoch [50/50], Step [261/531], Loss: 0.7402, tp_sum: 4.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.5000\n",
      "Epoch [50/50], Step [281/531], Loss: 0.6341, tp_sum: 5.0000, fp_sum: 4.0000, fn_sum: 2.0000, batch_f1_score: 0.6250\n",
      "Epoch [50/50], Step [301/531], Loss: 0.6515, tp_sum: 5.0000, fp_sum: 3.0000, fn_sum: 2.0000, batch_f1_score: 0.6667\n",
      "Epoch [50/50], Step [321/531], Loss: 0.6654, tp_sum: 7.0000, fp_sum: 2.0000, fn_sum: 4.0000, batch_f1_score: 0.7000\n",
      "Epoch [50/50], Step [341/531], Loss: 0.6581, tp_sum: 7.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.7778\n",
      "Epoch [50/50], Step [361/531], Loss: 0.7073, tp_sum: 3.0000, fp_sum: 5.0000, fn_sum: 4.0000, batch_f1_score: 0.4000\n",
      "Epoch [50/50], Step [381/531], Loss: 0.5979, tp_sum: 7.0000, fp_sum: 0.0000, fn_sum: 4.0000, batch_f1_score: 0.7778\n",
      "Epoch [50/50], Step [401/531], Loss: 0.6315, tp_sum: 7.0000, fp_sum: 0.0000, fn_sum: 3.0000, batch_f1_score: 0.8235\n",
      "Epoch [50/50], Step [421/531], Loss: 0.7022, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 4.0000, batch_f1_score: 0.6000\n",
      "Epoch [50/50], Step [441/531], Loss: 0.7435, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [50/50], Step [461/531], Loss: 0.6412, tp_sum: 4.0000, fp_sum: 4.0000, fn_sum: 3.0000, batch_f1_score: 0.5333\n",
      "Epoch [50/50], Step [481/531], Loss: 0.6031, tp_sum: 9.0000, fp_sum: 1.0000, fn_sum: 3.0000, batch_f1_score: 0.8182\n",
      "Epoch [50/50], Step [501/531], Loss: 0.7254, tp_sum: 3.0000, fp_sum: 6.0000, fn_sum: 2.0000, batch_f1_score: 0.4286\n",
      "Epoch [50/50], Step [521/531], Loss: 0.6227, tp_sum: 6.0000, fp_sum: 4.0000, fn_sum: 1.0000, batch_f1_score: 0.7059\n"
     ]
    }
   ],
   "source": [
    "train(model = model,\n",
    "      train_loader = train_loader,\n",
    "      train_dataset_length = len(train_dataset),\n",
    "      val_loader = val_loader,\n",
    "      num_class = num_class,\n",
    "      device = device,\n",
    "      model_name = \"custom_dnet_binary_by_img_count_lr_1e-4_long\",\n",
    "      lr = 1e-4\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
