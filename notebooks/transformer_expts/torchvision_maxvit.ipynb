{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "# device = \"cpu\"\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..\\\\chexnet', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\notebooks', 'C:\\\\Python312\\\\python312.zip', 'C:\\\\Python312\\\\DLLs', 'C:\\\\Python312\\\\Lib', 'C:\\\\Python312', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv', '', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# run the below line once only\n",
    "if \"..\\\\chexnet\" not in sys.path:\n",
    "    sys.path.insert(0,r'..\\chexnet')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetGenerator import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 2048 images from C:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\chexnet\\dataset\\train_1.txt\n",
      "Collected 2048 images from C:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\chexnet\\dataset\\val_1.txt\n"
     ]
    }
   ],
   "source": [
    "pathDirData = r'C:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\raw_data\\archive'\n",
    "pathFileTrain = r'C:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\chexnet\\dataset\\train_1.txt'\n",
    "pathFileVal = r'C:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\chexnet\\dataset\\val_1.txt'\n",
    "\n",
    "transResize = 256\n",
    "transCrop = 224\n",
    "trBatchSize = 16\n",
    "num_class = 14\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "transformList = []\n",
    "transformList.append(transforms.Resize(transResize))\n",
    "transformList.append(transforms.RandomResizedCrop(transCrop))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence=transforms.Compose(transformList)\n",
    "\n",
    "datasetTrain = DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileTrain, transform=transformSequence)\n",
    "datasetVal =   DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileVal, transform=transformSequence)\n",
    "train_loader = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=12, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=datasetVal, batch_size=trBatchSize, shuffle=False, num_workers=12, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call MaxVit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import maxvit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaxVit_T_Weights.IMAGENET1K_V1`. You can also use `weights=MaxVit_T_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x0000017F7F2E0D60>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"c:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1437, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaxVit(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): Conv2dNormActivation(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): MaxVitBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Sequential(\n",
       "                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (stochastic_depth): Identity()\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (merge): Linear(in_features=64, out_features=64, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.0, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (merge): Linear(in_features=64, out_features=64, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.0, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (merge): Linear(in_features=64, out_features=64, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.02, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (merge): Linear(in_features=64, out_features=64, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.02, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MaxVitBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Sequential(\n",
       "                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "                (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (merge): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.04, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (merge): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.04, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.06, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (merge): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.06, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (merge): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.06, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MaxVitBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Sequential(\n",
       "                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "                (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.08, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.08, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.1, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.1, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.12, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.12, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.12, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.14, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.14, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.16, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.16, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MaxVitBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Sequential(\n",
       "                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "                (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=2048, bias=False)\n",
       "                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (merge): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.18, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (merge): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.18, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (merge): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.2, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (merge): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.2, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=14, bias=True)\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialise first layer\n",
    "model = maxvit.maxvit_t(\n",
    "    weights = maxvit.MaxVit_T_Weights,\n",
    "    input_size = (transCrop, transCrop)\n",
    ")\n",
    "##  Freeze earlier weights\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad= False\n",
    "\n",
    "##  Replace the last layer \n",
    "model.classifier[-1] = nn.Sequential(\n",
    "    nn.Linear(in_features=model.classifier[5].in_features,out_features= num_class),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "## set to cuda\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[-1][-0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate the F1 score\n",
    "def f1_score(tp, fp, fn):\n",
    "    return 2 * (tp) / (2 * tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 5\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i, (images, labels, _) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        print(images.shape)\n",
    "        break\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # calculate statistics\n",
    "        # pred_labels = (nn.Softmax(dim=1)(outputs) > 1/14).long()\n",
    "        \n",
    "        tp_array = [0 for x in range(num_class)]\n",
    "        fp_array = [0 for x in range(num_class)]\n",
    "        fn_array = [0 for x in range(num_class)]\n",
    "        pred_labels = (outputs > 1/4).long()\n",
    "        tp_array += sum(torch.logical_and(pred_labels, labels))\n",
    "        fp_array += sum(torch.logical_and(torch.logical_xor(pred_labels, labels).long(), pred_labels))\n",
    "        fn_array += sum(torch.logical_and(torch.logical_xor(pred_labels, labels).long(), labels))\n",
    "        \n",
    "        writer.add_scalar('Loss/train', loss, epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('TP_Sum/train', sum(tp_array), epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('FP_Sum/train', sum(fp_array), epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('FN_Sum/train', sum(fn_array), epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('F1_Score/train', f1_score(sum(tp_array), sum(fp_array), sum(fn_array)), epoch * len(train_loader) + i)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # Display\n",
    "        # if (i + 1) % 100 == 0:\n",
    "        print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, tp_sum: {:.4f}, fp_sum: {:.4f}, fn_sum: {:.4f}, cumulative_f1_score: {:.4f}\".format(epoch + 1, \\\n",
    "                                                                     n_epochs, \\\n",
    "                                                                     i + 1, \\\n",
    "                                                                     len(train_loader), \\\n",
    "                                                                     loss,\\\n",
    "                                                                     sum(tp_array), \\\n",
    "                                                                     sum(fp_array),\\\n",
    "                                                                     sum(fn_array),\\\n",
    "                                                                     f1_score(sum(tp_array), sum(fp_array), sum(fn_array))))\n",
    "        print(\"outputs\\n\", outputs)\n",
    "        print(\"pred_labels\\n\", pred_labels)\n",
    "        print(\"actual labels\\n\", labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9449106487222299"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log(0.143)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
