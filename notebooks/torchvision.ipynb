{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "# device = \"cpu\"\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\chexnet', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\notebooks', 'C:\\\\Python312\\\\python312.zip', 'C:\\\\Python312\\\\DLLs', 'C:\\\\Python312\\\\Lib', 'C:\\\\Python312', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv', '', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\siyang\\\\Documents\\\\GitHub\\\\DeepLearningProject\\\\.venv\\\\Lib\\\\site-packages\\\\Pythonwin']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# run the below line once only\n",
    "# sys.path.insert(0,r'C:\\Users\\siyang\\Documents\\GitHub\\chexnet')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetGenerator import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathDirData = r'C:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\raw_data\\archive'\n",
    "pathFileTrain = r'C:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\chexnet\\dataset\\train_1.txt'\n",
    "pathFileVal = r'C:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\chexnet\\dataset\\val_1.txt'\n",
    "\n",
    "transResize = 256\n",
    "transCrop = 224\n",
    "trBatchSize = 128\n",
    "num_class = 14\n",
    "\n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "transformList = []\n",
    "# transformList.append(transforms.Resize(transResize))\n",
    "transformList.append(transforms.RandomResizedCrop(transCrop))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence=transforms.Compose(transformList)\n",
    "\n",
    "datasetTrain = DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileTrain, transform=transformSequence)\n",
    "datasetVal =   DatasetGenerator(pathImageDirectory=pathDirData, pathDatasetFile=pathFileVal, transform=transformSequence)\n",
    "train_loader = DataLoader(dataset=datasetTrain, batch_size=trBatchSize, shuffle=True,  num_workers=12, pin_memory=True)\n",
    "val_loader = DataLoader(dataset=datasetVal, batch_size=trBatchSize, shuffle=False, num_workers=12, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call MaxVit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import maxvit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaxVit_T_Weights.IMAGENET1K_V1`. You can also use `weights=MaxVit_T_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MaxVit(\n",
       "  (stem): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "    )\n",
       "    (1): Conv2dNormActivation(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): MaxVitBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Sequential(\n",
       "                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "                (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (stochastic_depth): Identity()\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (merge): Linear(in_features=64, out_features=64, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.0, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (merge): Linear(in_features=64, out_features=64, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.0, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "                  (1): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (merge): Linear(in_features=64, out_features=64, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.02, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=64, out_features=192, bias=True)\n",
       "                  (merge): Linear(in_features=64, out_features=64, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=64, out_features=256, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.02, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): MaxVitBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Sequential(\n",
       "                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "                (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512, bias=False)\n",
       "                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (merge): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.04, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (merge): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.04, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.06, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "                  (1): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (merge): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.06, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "                  (merge): Linear(in_features=128, out_features=128, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.06, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): MaxVitBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Sequential(\n",
       "                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "                (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(128, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.08, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.08, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.1, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.1, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.12, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.12, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.12, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.14, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.14, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)\n",
       "                  (1): BatchNorm2d(1024, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(1024, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(64, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.16, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "                  (merge): Linear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.16, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): MaxVitBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Sequential(\n",
       "                (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "                (1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "              (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(256, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=2048, bias=False)\n",
       "                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (merge): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.18, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (merge): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.18, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): MaxVitLayer(\n",
       "          (layers): Sequential(\n",
       "            (MBconv): MBConv(\n",
       "              (proj): Identity()\n",
       "              (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "              (layers): Sequential(\n",
       "                (pre_norm): BatchNorm2d(512, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                (conv_a): Conv2dNormActivation(\n",
       "                  (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (conv_b): Conv2dNormActivation(\n",
       "                  (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)\n",
       "                  (1): BatchNorm2d(2048, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
       "                  (2): GELU(approximate='none')\n",
       "                )\n",
       "                (squeeze_excitation): SqueezeExcitation(\n",
       "                  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "                  (fc1): Conv2d(2048, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (fc2): Conv2d(128, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "                  (activation): SiLU()\n",
       "                  (scale_activation): Sigmoid()\n",
       "                )\n",
       "                (conv_c): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "              )\n",
       "            )\n",
       "            (window_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): Identity()\n",
       "              (departition_swap): Identity()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (merge): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.2, mode=row)\n",
       "            )\n",
       "            (grid_attention): PartitionAttentionLayer(\n",
       "              (partition_op): WindowPartition()\n",
       "              (departition_op): WindowDepartition()\n",
       "              (partition_swap): SwapAxes()\n",
       "              (departition_swap): SwapAxes()\n",
       "              (attn_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): RelativePositionalMultiHeadAttention(\n",
       "                  (to_qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                  (merge): Linear(in_features=512, out_features=512, bias=True)\n",
       "                )\n",
       "                (2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (mlp_layer): Sequential(\n",
       "                (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                (1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (2): GELU(approximate='none')\n",
       "                (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (4): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (stochastic_dropout): StochasticDepth(p=0.2, mode=row)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): AdaptiveAvgPool2d(output_size=1)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (4): Tanh()\n",
       "    (5): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=14, bias=True)\n",
       "      (1): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Initialise first layer\n",
    "model = maxvit.maxvit_t(\n",
    "    weights = maxvit.MaxVit_T_Weights,\n",
    "    input_size = (transCrop, transCrop)\n",
    ")\n",
    "##  Freeze earlier weights\n",
    "for param in model.parameters():\n",
    "    param.requires_grad= False\n",
    "\n",
    "##  Replace the last layer \n",
    "model.classifier[-1] = nn.Sequential(\n",
    "    nn.Linear(in_features=model.classifier[5].in_features,out_features= num_class),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "## set to cuda\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier[-3].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to calculate the F1 score\n",
    "def f1_score(tp, fp, fn):\n",
    "    return 2 * (tp) / (2 * tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/614], Loss: 0.2049, tp_sum: 0.0000, fp_sum: 0.0000, fn_sum: 90.0000, cumulative_f1_score: 0.0000\n",
      "Epoch [1/5], Step [2/614], Loss: 0.2065, tp_sum: 0.0000, fp_sum: 1.0000, fn_sum: 98.0000, cumulative_f1_score: 0.0000\n",
      "Epoch [1/5], Step [3/614], Loss: 0.1919, tp_sum: 11.0000, fp_sum: 74.0000, fn_sum: 81.0000, cumulative_f1_score: 0.1243\n",
      "Epoch [1/5], Step [4/614], Loss: 0.1945, tp_sum: 20.0000, fp_sum: 113.0000, fn_sum: 73.0000, cumulative_f1_score: 0.1770\n",
      "Epoch [1/5], Step [5/614], Loss: 0.1868, tp_sum: 18.0000, fp_sum: 81.0000, fn_sum: 72.0000, cumulative_f1_score: 0.1905\n",
      "Epoch [1/5], Step [6/614], Loss: 0.2081, tp_sum: 19.0000, fp_sum: 73.0000, fn_sum: 87.0000, cumulative_f1_score: 0.1919\n",
      "Epoch [1/5], Step [7/614], Loss: 0.1796, tp_sum: 23.0000, fp_sum: 70.0000, fn_sum: 64.0000, cumulative_f1_score: 0.2556\n",
      "Epoch [1/5], Step [8/614], Loss: 0.1839, tp_sum: 20.0000, fp_sum: 79.0000, fn_sum: 76.0000, cumulative_f1_score: 0.2051\n",
      "Epoch [1/5], Step [9/614], Loss: 0.2165, tp_sum: 14.0000, fp_sum: 99.0000, fn_sum: 91.0000, cumulative_f1_score: 0.1284\n",
      "Epoch [1/5], Step [10/614], Loss: 0.1768, tp_sum: 19.0000, fp_sum: 70.0000, fn_sum: 63.0000, cumulative_f1_score: 0.2222\n",
      "Epoch [1/5], Step [11/614], Loss: 0.2135, tp_sum: 16.0000, fp_sum: 81.0000, fn_sum: 93.0000, cumulative_f1_score: 0.1553\n",
      "Epoch [1/5], Step [12/614], Loss: 0.2003, tp_sum: 18.0000, fp_sum: 74.0000, fn_sum: 80.0000, cumulative_f1_score: 0.1895\n",
      "Epoch [1/5], Step [13/614], Loss: 0.2118, tp_sum: 17.0000, fp_sum: 59.0000, fn_sum: 84.0000, cumulative_f1_score: 0.1921\n",
      "Epoch [1/5], Step [14/614], Loss: 0.1814, tp_sum: 8.0000, fp_sum: 63.0000, fn_sum: 72.0000, cumulative_f1_score: 0.1060\n",
      "Epoch [1/5], Step [15/614], Loss: 0.1959, tp_sum: 9.0000, fp_sum: 60.0000, fn_sum: 84.0000, cumulative_f1_score: 0.1111\n",
      "Epoch [1/5], Step [16/614], Loss: 0.2154, tp_sum: 10.0000, fp_sum: 58.0000, fn_sum: 100.0000, cumulative_f1_score: 0.1124\n",
      "Epoch [1/5], Step [17/614], Loss: 0.2115, tp_sum: 11.0000, fp_sum: 42.0000, fn_sum: 94.0000, cumulative_f1_score: 0.1392\n",
      "Epoch [1/5], Step [18/614], Loss: 0.2056, tp_sum: 11.0000, fp_sum: 65.0000, fn_sum: 84.0000, cumulative_f1_score: 0.1287\n",
      "Epoch [1/5], Step [19/614], Loss: 0.2043, tp_sum: 14.0000, fp_sum: 46.0000, fn_sum: 94.0000, cumulative_f1_score: 0.1667\n",
      "Epoch [1/5], Step [20/614], Loss: 0.1951, tp_sum: 14.0000, fp_sum: 69.0000, fn_sum: 84.0000, cumulative_f1_score: 0.1547\n",
      "Epoch [1/5], Step [21/614], Loss: 0.1564, tp_sum: 15.0000, fp_sum: 84.0000, fn_sum: 56.0000, cumulative_f1_score: 0.1765\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m fp_array \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlogical_and(torch\u001b[38;5;241m.\u001b[39mlogical_xor(pred_labels, labels)\u001b[38;5;241m.\u001b[39mlong(), pred_labels))\n\u001b[0;32m     30\u001b[0m fn_array \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(torch\u001b[38;5;241m.\u001b[39mlogical_and(torch\u001b[38;5;241m.\u001b[39mlogical_xor(pred_labels, labels)\u001b[38;5;241m.\u001b[39mlong(), labels))\n\u001b[1;32m---> 32\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLoss/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTP_Sum/train\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28msum\u001b[39m(tp_array), epoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m+\u001b[39m i)\n\u001b[0;32m     34\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFP_Sum/train\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28msum\u001b[39m(fp_array), epoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader) \u001b[38;5;241m+\u001b[39m i)\n",
      "File \u001b[1;32mc:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:393\u001b[0m, in \u001b[0;36mSummaryWriter.add_scalar\u001b[1;34m(self, tag, scalar_value, global_step, walltime, new_style, double_precision)\u001b[0m\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcaffe2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m workspace\n\u001b[0;32m    391\u001b[0m     scalar_value \u001b[38;5;241m=\u001b[39m workspace\u001b[38;5;241m.\u001b[39mFetchBlob(scalar_value)\n\u001b[1;32m--> 393\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43mscalar\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalar_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_style\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_style\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdouble_precision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdouble_precision\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(summary, global_step, walltime)\n",
      "File \u001b[1;32mc:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torch\\utils\\tensorboard\\summary.py:369\u001b[0m, in \u001b[0;36mscalar\u001b[1;34m(name, tensor, collections, new_style, double_precision)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mscalar\u001b[39m(name, tensor, collections\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, new_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, double_precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    353\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Output a `Summary` protocol buffer containing a single scalar value.\u001b[39;00m\n\u001b[0;32m    354\u001b[0m \n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    The generated Summary has a Tensor.proto containing the input Tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;124;03m      ValueError: If tensor has the wrong shape or type.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmake_np\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    371\u001b[0m         tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    372\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor should contain one element (0 dimensions). Was given size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;66;03m# python float is double precision in numpy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torch\\utils\\tensorboard\\_convert_np.py:23\u001b[0m, in \u001b[0;36mmake_np\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([x])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_prepare_pytorch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but numpy array, torch tensor, or caffe2 blob name are expected.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     26\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\siyang\\Documents\\GitHub\\DeepLearningProject\\.venv\\Lib\\site-packages\\torch\\utils\\tensorboard\\_convert_np.py:30\u001b[0m, in \u001b[0;36m_prepare_pytorch\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_prepare_pytorch\u001b[39m(x):\n\u001b[1;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "scheduler = ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n",
    "\n",
    "# Create a TensorBoard writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # calculate statistics\n",
    "        # pred_labels = (nn.Softmax(dim=1)(outputs) > 1/14).long()\n",
    "        \n",
    "        tp_array = [0 for x in range(num_class)]\n",
    "        fp_array = [0 for x in range(num_class)]\n",
    "        fn_array = [0 for x in range(num_class)]\n",
    "        pred_labels = (outputs > 1/4).long()\n",
    "        tp_array += sum(torch.logical_and(pred_labels, labels))\n",
    "        fp_array += sum(torch.logical_and(torch.logical_xor(pred_labels, labels).long(), pred_labels))\n",
    "        fn_array += sum(torch.logical_and(torch.logical_xor(pred_labels, labels).long(), labels))\n",
    "        \n",
    "        writer.add_scalar('Loss/train', loss, epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('TP_Sum/train', sum(tp_array), epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('FP_Sum/train', sum(fp_array), epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('FN_Sum/train', sum(fn_array), epoch * len(train_loader) + i)\n",
    "        writer.add_scalar('F1_Score/train', f1_score(sum(tp_array), sum(fp_array), sum(fn_array)), epoch * len(train_loader) + i)\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Display\n",
    "        # if (i + 1) % 100 == 0:\n",
    "        print(\"Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, tp_sum: {:.4f}, fp_sum: {:.4f}, fn_sum: {:.4f}, cumulative_f1_score: {:.4f}\".format(epoch + 1, \\\n",
    "                                                                     n_epochs, \\\n",
    "                                                                     i + 1, \\\n",
    "                                                                     len(train_loader), \\\n",
    "                                                                     loss,\\\n",
    "                                                                     sum(tp_array), \\\n",
    "                                                                     sum(fp_array),\\\n",
    "                                                                     sum(fn_array),\\\n",
    "                                                                     f1_score(sum(tp_array), sum(fp_array), sum(fn_array))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
